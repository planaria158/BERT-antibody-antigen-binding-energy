config:
  accelerator: gpu
  batch_size: 2048
  betas:
  - 0.9
  - 0.95
  checkpoint_every_n_train_steps: 3
  checkpoint_name: None
  devices: 2
  grad_norm_clip: 1.0
  inference_results_folder: ../inference_results/mlp_model/cleaned-3b-data/
  learning_rate: 0.001
  log_dir: ../lightning_logs/mlp_model/cleaned-3b-data/
  log_every_nsteps: 10
  lr_gamma: 0.9995
  mlp_dropout: 0.25
  mode: min
  monitor: val_loss
  num_epochs: 5000
  num_workers: 5
  save_top_k: 10
  seq_flip_prob: 0.2
  seq_mask_prob: 0.1
  sequence_regularize: true
  test_results_folder: ../test_results/mlp_model/cleaned-3b-data/
  train_data_path: ../data/q_cleaned_3b_train_set.csv
  val_data_path: ../data/q_cleaned_3b_val_set.csv
in_dim: 247
