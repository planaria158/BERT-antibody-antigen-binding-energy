config:
  accelerator: gpu
  batch_size: 2048
  betas:
  - 0.9
  - 0.95
  block_size: 247
  checkpoint_every_n_train_steps: 100
  devices: 2
  grad_norm_clip: 1.0
  learning_rate: 0.0001
  log_dir: ./lightning_logs/dense_mlp_model/
  log_every_nsteps: 10
  lr_gamma: 0.999
  mask_prob: 0.0
  mlp_dropout: 0.2
  mode: min
  monitor: loss
  num_epochs: 10000
  num_workers: 5
  save_top_k: 5
  seed: 3407
  test_data_path: /home/mark/dev/aAlphaBio-Homework/data/val_set.csv
  train_data_path: /home/mark/dev/aAlphaBio-Homework/data/train_set.csv
  vocab_size: 24
