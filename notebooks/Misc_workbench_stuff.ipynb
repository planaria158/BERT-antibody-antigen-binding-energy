{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/markthompson/Documents/dev/a-alphaBio-homework/notebooks', '/Users/markthompson/anaconda3/envs/avm-dvm/lib/python39.zip', '/Users/markthompson/anaconda3/envs/avm-dvm/lib/python3.9', '/Users/markthompson/anaconda3/envs/avm-dvm/lib/python3.9/lib-dynload', '', '/Users/markthompson/anaconda3/envs/avm-dvm/lib/python3.9/site-packages', '/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/']\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append('/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/') \n",
    "print(sys.path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m144\u001b[39m\n\u001b[1;32m      2\u001b[0m dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m----> 3\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, num_patches \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dim)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_patches = 144\n",
    "dim = 128\n",
    "a = torch.randn(1, num_patches + 1, dim)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2085529798.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    config_path = '../config/mlp_params.yaml'  |\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Read the config\n",
    "config_path = '../config/mlp_params.yaml'  |\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)\n",
    "\n",
    "# hardwire the block size to 265\n",
    "config['block_size'] = 265\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.inference = inference\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "        affinity = self.df.loc[idx, 'Kd'] if self.inference == False else 0.0\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert affinity >= 0.0, 'affinity cannot be negative'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) <= self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # print('chunk length:', len(chunk), ', chunk:', chunk)\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        first_aa = 1 # first aa position in the sequence (after CLS)\n",
    "        last_aa = dix.shape[0] # last aa position in the sequence\n",
    "        # print('first_aa:', first_aa, ', last_aa:', last_aa)\n",
    "        if dix.shape[0] < self.config['block_size']:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.config['block_size'] - len(dix)), dtype=torch.long)))\n",
    "\n",
    "        mask = None\n",
    "        if self.config['mask_prob'] > 0:\n",
    "            # dix looks like: [[CLS], x1, x2, x3, ..., xN, [PAD], [PAD], ..., [PAD]]\n",
    "            # Never mask CLS or PAD tokens\n",
    "\n",
    "            # get number of tokens to mask\n",
    "            n_pred = max(1, int(round((last_aa - first_aa)*self.config['mask_prob'])))\n",
    "\n",
    "            # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "            masked_idx = torch.randperm(last_aa-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "            # ... and overwrite them with MASK token in the data\n",
    "            dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, torch.tensor([affinity], dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, img_shape=(46,46), skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _make_img(self, x, shape=(46,46)):\n",
    "        d = ''.join([self._bin(x[i]) for i in x.numpy()])\n",
    "        # turn d into a list of integers, one for each bit\n",
    "        d = [int(x) for x in d]    \n",
    "        t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "        t = t.reshape(shape)\n",
    "        t = t.unsqueeze(0) # add channel dimension\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "        img = self._make_img(dix, self.img_shape)\n",
    "        # print('dataset: img shape:', img.shape, ', kd:', kd)\n",
    "        return img, kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset(config, train_data_path)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, kd = train_dataset.__getitem__(0)\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
