{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/markthompson/Documents/dev/a-alphaBio-homework/notebooks', '/Users/markthompson/anaconda3/envs/avm-dvm/lib/python39.zip', '/Users/markthompson/anaconda3/envs/avm-dvm/lib/python3.9', '/Users/markthompson/anaconda3/envs/avm-dvm/lib/python3.9/lib-dynload', '', '/Users/markthompson/anaconda3/envs/avm-dvm/lib/python3.9/site-packages', '/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/']\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append('/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/') \n",
    "print(sys.path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_size': 248, 'mask_prob': 0.0, 'mlp_dropout': 0.2, 'accelerator': 'gpu', 'devices': 1, 'batch_size': 2048, 'num_workers': 5, 'grad_norm_clip': 1.0, 'num_epochs': 10000, 'log_dir': './lightning_logs/mlp_model/', 'train_data_path': '../data/train_set.csv', 'test_data_path': '../data/val_set.csv', 'checkpoint_name': 'None', 'learning_rate': 0.0001, 'lr_gamma': 0.999, 'betas': [0.9, 0.95], 'checkpoint_every_n_train_steps': 100, 'save_top_k': 1, 'monitor': 'loss', 'mode': 'min', 'log_every_nsteps': 10, 'inference_results_folder': './inference_results/', 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "# Read the config\n",
    "config_path = '../config/mlp_params.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)\n",
    "\n",
    "# hardwire the block size to 265\n",
    "config['block_size'] = 265\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.inference = inference\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "        affinity = self.df.loc[idx, 'Kd'] if self.inference == False else 0.0\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert affinity >= 0.0, 'affinity cannot be negative'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) <= self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # print('chunk length:', len(chunk), ', chunk:', chunk)\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        first_aa = 1 # first aa position in the sequence (after CLS)\n",
    "        last_aa = dix.shape[0] # last aa position in the sequence\n",
    "        # print('first_aa:', first_aa, ', last_aa:', last_aa)\n",
    "        if dix.shape[0] < self.config['block_size']:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.config['block_size'] - len(dix)), dtype=torch.long)))\n",
    "\n",
    "        mask = None\n",
    "        if self.config['mask_prob'] > 0:\n",
    "            # dix looks like: [[CLS], x1, x2, x3, ..., xN, [PAD], [PAD], ..., [PAD]]\n",
    "            # Never mask CLS or PAD tokens\n",
    "\n",
    "            # get number of tokens to mask\n",
    "            n_pred = max(1, int(round((last_aa - first_aa)*self.config['mask_prob'])))\n",
    "\n",
    "            # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "            masked_idx = torch.randperm(last_aa-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "            # ... and overwrite them with MASK token in the data\n",
    "            dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, torch.tensor([affinity], dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _make_img(self, x, shape=(46,46)):\n",
    "        d = ''.join([self._bin(x[i]) for i in x.numpy()])\n",
    "        # turn d into a list of integers, one for each bit\n",
    "        d = [int(x) for x in d]    \n",
    "        t = torch.tensor(d[:(46*46)], dtype=torch.float32) # this is for 46,46 matrix\n",
    "        t = t.reshape(shape)\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "        img = self._make_img(dix)\n",
    "        return img, kd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data from: ../data/train_set.csv\n",
      "vocabulary: ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
      "data has 26279 rows, 24 vocab size (unique).\n",
      "26279\n",
      "config[vocab_size]: 24 , config[block_size]: 265\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset(config, train_data_path)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img shape: torch.Size([46, 46]) , kd: tensor([1.5350])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x175519f40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, kd = train_dataset.__getitem__(0)\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
