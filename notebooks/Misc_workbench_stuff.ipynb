{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mark/dev/aAlphaBio-Homework/notebooks', '/home/mark/anaconda3/envs/avm-dvm/lib/python39.zip', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/lib-dynload', '', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages', '/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/']\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append('/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/') \n",
    "print(sys.path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 145, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_patches = 144\n",
    "dim = 128\n",
    "a = torch.randn(1, num_patches + 1, dim)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_size': 288, 'mask_prob': 0.0, 'image_shape': [48, 48], 'patch_dim': 4, 'num_heads': 8, 'num_layers': 6, 'dim': 128, 'dropout': 0.3, 'accelerator': 'mps', 'devices': 1, 'batch_size': 512, 'num_workers': 10, 'grad_norm_clip': 1.0, 'num_epochs': 1000, 'log_dir': './lightning_logs/vit_model/cleaned-2/', 'train_data_path': '../data/train_set.csv', 'test_data_path': 'None', 'inference_data_path': '/Users/markthompson/Documents/dev/a-alphaBio-homework/data/q_cleaned_2_val_set.csv', 'checkpoint_name': '/Users/markthompson/Documents/dev/a-alphaBio-homework/lightning_logs/vit_model/cleaned-2/trained/checkpoints/epoch=836-step=18400.ckpt', 'learning_rate': 0.0001, 'lr_gamma': 0.999, 'betas': [0.9, 0.95], 'checkpoint_every_n_train_steps': 100, 'save_top_k': 4, 'monitor': 'val_loss', 'mode': 'min', 'log_every_nsteps': 10, 'inference_results_folder': './inference_results/vit/cleaned-2/', 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "# Read the config\n",
    "config_path = '../config/vit_params.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.inference = inference\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "        affinity = self.df.loc[idx, 'Kd'] if self.inference == False else 0.0\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert affinity >= 0.0, 'affinity cannot be negative'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) <= self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # print('chunk length:', len(chunk), ', chunk:', chunk)\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        first_aa = 1 # first aa position in the sequence (after CLS)\n",
    "        last_aa = dix.shape[0] # last aa position in the sequence\n",
    "        # print('first_aa:', first_aa, ', last_aa:', last_aa)\n",
    "        if dix.shape[0] < self.config['block_size']:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.config['block_size'] - len(dix)), dtype=torch.long)))\n",
    "\n",
    "        mask = None\n",
    "        if self.config['mask_prob'] > 0:\n",
    "            # dix looks like: [[CLS], x1, x2, x3, ..., xN, [PAD], [PAD], ..., [PAD]]\n",
    "            # Never mask CLS or PAD tokens\n",
    "\n",
    "            # get number of tokens to mask\n",
    "            n_pred = max(1, int(round((last_aa - first_aa)*self.config['mask_prob'])))\n",
    "\n",
    "            # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "            masked_idx = torch.randperm(last_aa-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "            # ... and overwrite them with MASK token in the data\n",
    "            dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, torch.tensor([affinity], dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets.scFv_dataset import scFv_Dataset\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Simple wrapper Dataset to turn output from the scFv dataset\n",
    "# into a B&W image for use in a CNN model\n",
    "#--------------------------------------------------------\n",
    "class CNN_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        self.img_shape = config['image_shape']\n",
    "         \n",
    "        chars = self.scFv_dataset.chars\n",
    "        groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "                'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "        \n",
    "        # for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "        group_encodings = { 'none'    : int('00101000', base=2), \n",
    "                            'polar'   : int('00110011', base=2),\n",
    "                            'nonpolar': int('11001100', base=2), \n",
    "                            'pos'     : int('01010101', base=2),\n",
    "                            'neg'     : int('10101010', base=2)} \n",
    "\n",
    "        # map encoded sequence to groups\n",
    "        self.i_to_grp = {self.scFv_dataset.stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "        print('i_to_grp:', self.i_to_grp)\n",
    "\n",
    "        # The normalized frequence for each amino acid position in the scFv sequences over the entire clean_3 dataset\n",
    "        # This fixed-array is 246 elements long.\n",
    "        self.norm_mutation_freq = torch.tensor([0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.95, \n",
    "                            0.95, 0.95, 0.95, 0.95, 0.95, 0.75, 0.9, 0.9, 0.95, 0.95, 0.95, 0.9, 0.95, 0.95, 0.85, 0.55,\n",
    "                            0.5, 0.25, 0.35, 0.3, 0.6, 0.45, 0.55, 0.95, 0.8, 0.8, 0.8, 0.9, 0.75, 0.8, 0.6, 0.95, 0.85, \n",
    "                            0.35, 0.9, 0.85, 0.15, 0.4, 0.9, 0.3, 0.9, 0.8, 0.5, 0.95, 0.95, 0.95, 0.9, 0.6, 0.6, 0.75, \n",
    "                            0.3, 0.85, 0.4, 0.9, 0.95, 0.4, 0.9, 0.95, 0.9, 0.25, 0.75, 0.55, 0.85, 0.25, 0.75, 0.6, 0.65, \n",
    "                            0.8, 0.8, 0.95, 0.95, 0.85, 0.8, 0.8, 0.9, 0.7, 0.9, 0.9, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, \n",
    "                            0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
    "                            0.05, 0.05, 0.05, 0.05])\n",
    "        \n",
    "        # promote values to 8-bit value range (255), then round up the norm_mutation_freq_encoded tensor and convert to long\n",
    "        self.norm_mutation_freq_encoded = self.norm_mutation_freq * 255\n",
    "        self.norm_mutation_freq_encoded = torch.round(self.norm_mutation_freq_encoded).to(torch.long)\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _encode_channel(self, x, shape=(48,48)):\n",
    "        d = ''.join([self._bin(x[i]) for i in x.numpy()])\n",
    "        # turn d into a list of integers, one for each bit\n",
    "        d = [int(x) for x in d]    \n",
    "        t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "        t = t.reshape(shape)\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "\n",
    "        # The residue encoding channel\n",
    "        ch_1 = self._encode_channel(dix, self.img_shape)\n",
    "\n",
    "        # The residue group encoding channel\n",
    "        dix_grp = torch.tensor([self.i_to_grp[i] for i in dix.numpy().tolist()], dtype=torch.long)\n",
    "        ch_2 = self._encode_channel(dix_grp, self.img_shape)\n",
    "\n",
    "        # The mutation frequency channel\n",
    "        # anything not an amino acid gets a zero.\n",
    "        ch3_in = torch.zeros_like(dix)\n",
    "        # First aa is always position 1 (0 is a CLS token)\n",
    "        ch3_in[1:len(self.norm_mutation_freq_encoded)+1] = self.norm_mutation_freq_encoded\n",
    "        ch_3 = self._encode_channel(ch3_in)\n",
    "\n",
    "        # stack the 3 channels into an bgr image\n",
    "        bgr_img = torch.stack((ch_1, ch_2, ch_3), dim=0)\n",
    "\n",
    "        return bgr_img, kd, ch_1, ch_2, ch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data from: ../data/train_set.csv\n",
      "vocabulary: ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
      "data has 26279 rows, 24 vocab size (unique).\n",
      "i_to_grp: {0: 40, 1: 204, 2: 204, 3: 170, 4: 170, 5: 204, 6: 204, 7: 85, 8: 204, 9: 85, 10: 204, 11: 204, 12: 170, 13: 204, 14: 170, 15: 85, 16: 51, 17: 51, 18: 204, 19: 204, 20: 51, 21: 40, 22: 40, 23: 40}\n",
      "26279\n",
      "config[vocab_size]: 24 , config[block_size]: 288\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset(config, train_data_path)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "img shape: torch.Size([3, 48, 48]) , kd: tensor([3.7660])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f06535e0e20>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, kd, ch_1, ch_2, ch_3 = train_dataset.__getitem__(105)\n",
    "print(img.dtype)\n",
    "# change the order of the channels to be (H, W) instead of (C, H, W)\n",
    "rgb_img = img.permute(1, 2, 0)\n",
    "\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "plt.imshow(rgb_img) #, cmap='gray')\n",
    "\n",
    "# plt.imshow(ch_3, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Examine other data that may be added as input channels to Vision Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the amino acids into their usual groups\n",
    "# polar, nonpolar, positively charged, negatively charged, or none (i.e. CLS, SEP, PAD)\n",
    "#\n",
    "# 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "# 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "         'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "print('\\nvocabulary:', chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "group_encodings = { 'none'    : int('00101000', base=2), \n",
    "                    'polar'   : int('00110011', base=2),\n",
    "                    'nonpolar': int('11001100', base=2), \n",
    "                    'pos'     : int('01010101', base=2),\n",
    "                    'neg'     : int('10101010', base=2)} \n",
    "\n",
    "print('group_encodings:', group_encodings)\n",
    "\n",
    "# maps amino acid to group                     \n",
    "s_to_grp = {ch:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "# maps encoded residue to group\n",
    "i_to_grp = {stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "print('\\ns_to_grp:', s_to_grp)\n",
    "print('\\ni_to_grp:', i_to_grp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _bin(x):\n",
    "    return format(x, '08b')\n",
    "\n",
    "def _encode_channel(x, shape=(48,48)):\n",
    "    d = ''.join([_bin(x[i]) for i in x.numpy()])\n",
    "    # turn d into a list of integers, one for each bit\n",
    "    d = [int(x) for x in d]    \n",
    "    t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "    t = t.reshape(shape)\n",
    "    # t = t.unsqueeze(0) # add channel dimension\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a heat-map for the variability of each position in the sequence?\n",
    "# That somehow changes with each sequence?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
