{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mark/dev/aAlphaBio-Homework/notebooks', '/home/mark/anaconda3/envs/avm-dvm/lib/python39.zip', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/lib-dynload', '', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages', '/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/']\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append('/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/') \n",
    "print(sys.path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 145, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_patches = 144\n",
    "dim = 128\n",
    "a = torch.randn(1, num_patches + 1, dim)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_size': 242, 'mask_prob': 0.0, 'image_shape': [44, 44], 'patch_dim': 4, 'num_heads': 8, 'num_layers': 6, 'dim': 128, 'dropout': 0.3, 'image_channels': 1, 'accelerator': 'gpu', 'devices': 2, 'batch_size': 512, 'num_workers': 10, 'grad_norm_clip': 1.0, 'num_epochs': 1000, 'log_dir': './lightning_logs/vit_model/cleaned-3/BW/', 'train_data_path': '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv', 'test_data_path': '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_val_set.csv', 'checkpoint_name': 'None', 'learning_rate': 0.0001, 'lr_gamma': 0.999, 'betas': [0.9, 0.95], 'checkpoint_every_n_train_steps': 100, 'save_top_k': 4, 'monitor': 'val_loss', 'mode': 'min', 'log_every_nsteps': 10, 'inference_results_folder': './inference_results/vit/cleaned-3/', 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "# Read the config\n",
    "config_path = '../config/vit_params.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.inference = inference\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "        affinity = self.df.loc[idx, 'Kd'] if self.inference == False else 0.0\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert affinity >= 0.0, 'affinity cannot be negative'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) <= self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # print('chunk length:', len(chunk), ', chunk:', chunk)\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        first_aa = 1 # first aa position in the sequence (after CLS)\n",
    "        last_aa = dix.shape[0] # last aa position in the sequence\n",
    "        # print('first_aa:', first_aa, ', last_aa:', last_aa)\n",
    "        if dix.shape[0] < self.config['block_size']:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.config['block_size'] - len(dix)), dtype=torch.long)))\n",
    "\n",
    "        mask = None\n",
    "        if self.config['mask_prob'] > 0:\n",
    "            # dix looks like: [[CLS], x1, x2, x3, ..., xN, [PAD], [PAD], ..., [PAD]]\n",
    "            # Never mask CLS or PAD tokens\n",
    "\n",
    "            # get number of tokens to mask\n",
    "            n_pred = max(1, int(round((last_aa - first_aa)*self.config['mask_prob'])))\n",
    "\n",
    "            # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "            masked_idx = torch.randperm(last_aa-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "            # ... and overwrite them with MASK token in the data\n",
    "            dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, torch.tensor([affinity], dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets.scFv_dataset import scFv_Dataset\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Simple wrapper Dataset to turn output from the scFv dataset\n",
    "# into a B&W image for use in a CNN model\n",
    "#--------------------------------------------------------\n",
    "class CNN_Dataset_BGR(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, transform=None, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        self.img_shape = config['image_shape']\n",
    "        self.transform = transform\n",
    "         \n",
    "        chars = self.scFv_dataset.chars\n",
    "        groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "                'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "        \n",
    "        # for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "        group_encodings = { 'none'    : int('11001100', base=2), \n",
    "                            'polar'   : int('00110011', base=2),\n",
    "                            'nonpolar': int('01100110', base=2), \n",
    "                            'pos'     : int('01010101', base=2),\n",
    "                            'neg'     : int('10101010', base=2)} \n",
    "        \n",
    "        print('group_encodings:', group_encodings)\n",
    "\n",
    "        # map encoded sequence to groups\n",
    "        self.i_to_grp = {self.scFv_dataset.stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "        print('i_to_grp:', self.i_to_grp)\n",
    "\n",
    "        # The relative mutation frequence for each amino acid position in the scFv sequences over the entire clean_3 dataset\n",
    "        # This fixed-array is 241 elements long.\n",
    "        self.rel_mutation_freq = torch.tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7778,\n",
    "                                                0.9444, 0.9444, 1.0000, 1.0000, 1.0000, 0.9444, 1.0000, 1.0000, 0.8889,\n",
    "                                                0.5556, 0.5000, 0.2222, 0.3333, 0.2778, 0.6111, 0.4444, 0.5556, 1.0000,\n",
    "                                                0.8333, 0.8333, 0.8333, 0.9444, 0.7778, 0.8333, 0.6111, 1.0000, 0.8889,\n",
    "                                                0.3333, 0.9444, 0.8889, 0.1111, 0.3889, 0.9444, 0.2778, 0.9444, 0.8333,\n",
    "                                                0.5000, 1.0000, 1.0000, 1.0000, 0.9444, 0.6111, 0.6111, 0.7778, 0.2778,\n",
    "                                                0.8889, 0.3889, 0.9444, 1.0000, 0.3889, 0.9444, 1.0000, 0.9444, 0.2222,\n",
    "                                                0.7778, 0.5556, 0.8889, 0.2222, 0.7778, 0.6111, 0.6667, 0.8333, 0.8333,\n",
    "                                                1.0000, 1.0000, 0.8889, 0.8333, 0.8333, 0.9444, 0.7222, 0.9444, 0.9444,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]) #, 0.0000, 0.0000,\n",
    "                                                #0.0000, 0.0000, 0.0000])\n",
    "        \n",
    "        print('len(rel_mutation_freq):', len(self.rel_mutation_freq))\n",
    "        \n",
    "        self.mutation_freq_encoded = self.rel_mutation_freq * 255\n",
    "        self.mutation_freq_encoded = torch.floor(self.mutation_freq_encoded).to(torch.long)\n",
    "        print(torch.min(self.mutation_freq_encoded), torch.max(self.mutation_freq_encoded))\n",
    "        print( self.mutation_freq_encoded[0:10])\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _encode_channel(self, x, shape):\n",
    "        d = ''.join([self._bin(val) for val in x])\n",
    "        # turn d into a list of integers, one for each bit\n",
    "        d = [int(x) for x in d]    \n",
    "        t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for shape matrix\n",
    "        t = t.reshape(shape)\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, Kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "\n",
    "        # 50% of the time flip the sequences back-to-front\n",
    "        flip = False\n",
    "        if random.random() > 0.5:\n",
    "            flip = True\n",
    "\n",
    "        dix = torch.flip(dix, [0]) if flip else dix\n",
    "\n",
    "        # The residue encoding channel\n",
    "        ch_1 = self._encode_channel(dix, self.img_shape)\n",
    "\n",
    "        # The residue group encoding channel\n",
    "        dix_grp = torch.tensor([self.i_to_grp[i] for i in dix.numpy().tolist()], dtype=torch.long)\n",
    "        dix_grp = torch.flip(dix_grp, [0]) if flip else dix_grp\n",
    "        ch_2 = self._encode_channel(dix_grp, self.img_shape)\n",
    "\n",
    "        # The mutation frequency channel; anything not an amino acid gets a zero.\n",
    "        ch3_in = torch.zeros_like(dix)\n",
    "        # First aa is always position 1 (0 is a CLS token)\n",
    "        ch3_in[1:len(self.mutation_freq_encoded)+1] = self.mutation_freq_encoded\n",
    "        ch3_in = torch.flip(ch3_in, [0]) if flip else ch3_in\n",
    "        ch_3 = self._encode_channel(ch3_in, self.img_shape)\n",
    "\n",
    "        # stack the 3 channels into an bgr image\n",
    "        bgr_img = torch.stack((ch_1, ch_2, ch_3), dim=0) * 255\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(bgr_img)\n",
    "            # Normalize image [-1, 1]\n",
    "            bgr_img = (bgr_img - 127.5)/127.5\n",
    "\n",
    "\n",
    "        return bgr_img, kd, ch_1, ch_2, ch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data from: /home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv\n",
      "vocabulary: ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
      "data has 8325 rows, 24 vocab size (unique).\n",
      "group_encodings: {'none': 204, 'polar': 51, 'nonpolar': 102, 'pos': 85, 'neg': 170}\n",
      "i_to_grp: {0: 204, 1: 102, 2: 102, 3: 170, 4: 170, 5: 102, 6: 102, 7: 85, 8: 102, 9: 85, 10: 102, 11: 102, 12: 170, 13: 102, 14: 170, 15: 85, 16: 51, 17: 51, 18: 102, 19: 102, 20: 51, 21: 204, 22: 204, 23: 204}\n",
      "len(rel_mutation_freq): 241\n",
      "tensor(0) tensor(255)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "8325\n",
      "config[vocab_size]: 24 , config[block_size]: 242\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "\n",
    "train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "                            RandomHorizontalFlip(p=0.25),\n",
    "                            RandomVerticalFlip(p=0.25)])\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset(config, train_data_path, train_transforms)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "img shape: torch.Size([3, 44, 44]) , kd: tensor([2.4634])\n",
      "tensor(-1.) ,  tensor(1.) ,  tensor(-0.3678) ,  tensor(0.9300)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, kd, ch_1, ch_2, ch_3 = train_dataset.__getitem__(105)\n",
    "print(img.dtype)\n",
    "# change the order of the channels to be (H, W) instead of (C, H, W)\n",
    "rgb_img = img.permute(1, 2, 0)\n",
    "\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "plt.imshow(rgb_img) #, cmap='gray')\n",
    "\n",
    "# plt.imshow(ch_3, cmap='gray')\n",
    "\n",
    "print(torch.min(rgb_img), ', ', torch.max(rgb_img), ', ', torch.mean(rgb_img), ', ', torch.std(rgb_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Examine other data that may be added as input channels to Vision Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the amino acids into their usual groups\n",
    "# polar, nonpolar, positively charged, negatively charged, or none (i.e. CLS, SEP, PAD)\n",
    "#\n",
    "# 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "# 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "         'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "print('\\nvocabulary:', chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "group_encodings = { 'none'    : int('00101000', base=2), \n",
    "                    'polar'   : int('00110011', base=2),\n",
    "                    'nonpolar': int('11001100', base=2), \n",
    "                    'pos'     : int('01010101', base=2),\n",
    "                    'neg'     : int('10101010', base=2)} \n",
    "\n",
    "print('group_encodings:', group_encodings)\n",
    "\n",
    "# maps amino acid to group                     \n",
    "s_to_grp = {ch:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "# maps encoded residue to group\n",
    "i_to_grp = {stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "print('\\ns_to_grp:', s_to_grp)\n",
    "print('\\ni_to_grp:', i_to_grp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _bin(x):\n",
    "    return format(x, '08b')\n",
    "\n",
    "def _encode_channel(x, shape=(48,48)):\n",
    "    d = ''.join([_bin(x[i]) for i in x.numpy()])\n",
    "    # turn d into a list of integers, one for each bit\n",
    "    d = [int(x) for x in d]    \n",
    "    t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "    t = t.reshape(shape)\n",
    "    # t = t.unsqueeze(0) # add channel dimension\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a heat-map for the variability of each position in the sequence?\n",
    "# That somehow changes with each sequence?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
