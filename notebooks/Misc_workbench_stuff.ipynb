{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mark/dev/aAlphaBio-Homework/notebooks', '/home/mark/anaconda3/envs/avm-dvm/lib/python39.zip', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/lib-dynload', '', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages', '/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/']\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append('/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/') \n",
    "print(sys.path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 145, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_patches = 144\n",
    "dim = 128\n",
    "a = torch.randn(1, num_patches + 1, dim)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_size': 242, 'seq_flip_prob': 0.1, 'mask_prob': 0.0, 'image_shape': [44, 44], 'patch_dim': 4, 'num_heads': 8, 'dim_head': 64, 'num_layers': 6, 'emb_dim': 256, 'dropout': 0.2, 'emb_dropout': 0.2, 'image_channels': 1, 'accelerator': 'gpu', 'devices': 2, 'batch_size': 384, 'num_workers': 10, 'grad_norm_clip': 1.0, 'num_epochs': 1000, 'log_dir': './lightning_logs/vit_model/cleaned-3/BW/', 'train_data_path': '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv', 'test_data_path': '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_val_set.csv', 'checkpoint_name': 'None', 'learning_rate': 0.0001, 'lr_gamma': 0.999, 'betas': [0.9, 0.95], 'checkpoint_every_n_train_steps': 100, 'save_top_k': 4, 'monitor': 'val_loss', 'mode': 'min', 'log_every_nsteps': 10, 'inference_results_folder': './inference_results/vit/cleaned-3/', 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "# Read the config\n",
    "config_path = '../config/vit_params.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.inference = inference\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "        affinity = self.df.loc[idx, 'Kd'] if self.inference == False else 0.0\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert affinity >= 0.0, 'affinity cannot be negative'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) <= self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # encode every character\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # Occasionally flip the aa sequences back-to-front to improve learning \n",
    "        dix = torch.flip(dix, [0]) if (random.random() < self.config['seq_flip_prob']) else dix\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        first_aa = 1 # first aa position in the sequence (after CLS)\n",
    "        last_aa = dix.shape[0] # last aa position in the sequence\n",
    "        if dix.shape[0] < self.config['block_size']:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.config['block_size'] - len(dix)), dtype=torch.long)))\n",
    "\n",
    "        # dix looks like: [[CLS], x1, x2, x3, ..., xN, [PAD], [PAD], ..., [PAD]]\n",
    "        # Never mask CLS or PAD tokens\n",
    "        mask = None\n",
    "        if self.config['mask_prob'] > 0:\n",
    "            # get number of tokens to mask\n",
    "            n_pred = max(1, int(round((last_aa - first_aa)*self.config['mask_prob'])))\n",
    "\n",
    "            # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "            masked_idx = torch.randperm(last_aa-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "            # ... and overwrite them with MASK token in the data\n",
    "            dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, torch.tensor([affinity], dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets.scFv_dataset import scFv_Dataset\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Simple wrapper Dataset to turn output from the scFv dataset\n",
    "# into a B&W image for use in a CNN model\n",
    "#--------------------------------------------------------\n",
    "class CNN_Dataset_BGR(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, transform=None, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        self.img_shape = config['image_shape']\n",
    "        self.transform = transform\n",
    "         \n",
    "        chars = self.scFv_dataset.chars\n",
    "        groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "                'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "        \n",
    "        # for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "        group_encodings = { 'none'    : int('11001100', base=2), \n",
    "                            'polar'   : int('00110011', base=2),\n",
    "                            'nonpolar': int('01100110', base=2), \n",
    "                            'pos'     : int('01010101', base=2),\n",
    "                            'neg'     : int('10101010', base=2)} \n",
    "        \n",
    "        print('group_encodings:', group_encodings)\n",
    "\n",
    "        # map encoded sequence to groups\n",
    "        self.i_to_grp = {self.scFv_dataset.stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "        # The relative mutation frequence for each amino acid position in the scFv sequences over the entire clean_3 dataset\n",
    "        # This fixed-array is 241 elements long. (I clipped off the last 5 residues from the 246 residue sequences for the VIT model)\n",
    "        self.rel_mutation_freq = torch.tensor([ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7778,\n",
    "                                                0.9444, 0.9444, 1.0000, 1.0000, 1.0000, 0.9444, 1.0000, 1.0000, 0.8889,\n",
    "                                                0.5556, 0.5000, 0.2222, 0.3333, 0.2778, 0.6111, 0.4444, 0.5556, 1.0000,\n",
    "                                                0.8333, 0.8333, 0.8333, 0.9444, 0.7778, 0.8333, 0.6111, 1.0000, 0.8889,\n",
    "                                                0.3333, 0.9444, 0.8889, 0.1111, 0.3889, 0.9444, 0.2778, 0.9444, 0.8333,\n",
    "                                                0.5000, 1.0000, 1.0000, 1.0000, 0.9444, 0.6111, 0.6111, 0.7778, 0.2778,\n",
    "                                                0.8889, 0.3889, 0.9444, 1.0000, 0.3889, 0.9444, 1.0000, 0.9444, 0.2222,\n",
    "                                                0.7778, 0.5556, 0.8889, 0.2222, 0.7778, 0.6111, 0.6667, 0.8333, 0.8333,\n",
    "                                                1.0000, 1.0000, 0.8889, 0.8333, 0.8333, 0.9444, 0.7222, 0.9444, 0.9444,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]) \n",
    "        \n",
    "        self.mutation_freq_encoded = self.rel_mutation_freq * 255\n",
    "        self.mutation_freq_encoded = torch.ceil(self.mutation_freq_encoded).to(torch.long)\n",
    "        print('min mutation freq:', torch.min(self.mutation_freq_encoded), 'max mutation freq:', torch.max(self.mutation_freq_encoded))\n",
    "        print('some mutation_freq_encoded values:', self.mutation_freq_encoded[60:70])\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _encode_channel(self, x, shape):\n",
    "        d = ''.join([self._bin(val) for val in x])\n",
    "        d = [int(x) for x in d] # turn d into a list of integers, one for each bit\n",
    "        t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for shape matrix\n",
    "        t = t.reshape(shape)\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, Kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "\n",
    "        # The residue encoding channel\n",
    "        ch_1 = self._encode_channel(dix, self.img_shape)\n",
    "        print('ch_1:', ch_1.shape, torch.min(ch_1), torch.max(ch_1))\n",
    "\n",
    "        # The residue group encoding channel\n",
    "        dix_grp = torch.tensor([self.i_to_grp[i] for i in dix.numpy().tolist()], dtype=torch.long)\n",
    "        ch_2 = self._encode_channel(dix_grp, self.img_shape)\n",
    "        print('ch_2:', ch_2.shape, torch.min(ch_2), torch.max(ch_2))   \n",
    "\n",
    "        # The mutation frequency channel; anything not an amino acid gets a zero.\n",
    "        ch3_in = torch.zeros_like(dix)\n",
    "        # First aa is always position 1 (0 is a CLS token)\n",
    "        ch3_in[1:len(self.mutation_freq_encoded)+1] = self.mutation_freq_encoded\n",
    "        ch_3 = self._encode_channel(ch3_in, self.img_shape)\n",
    "        print('ch_3:', ch_3.shape, torch.min(ch_3), torch.max(ch_3))   \n",
    "\n",
    "        # stack the 3 channels into a bgr image\n",
    "        bgr_img = torch.stack((ch_1, ch_2, ch_3), dim=0) * 255\n",
    "\n",
    "        if self.transform:\n",
    "            bgr_img = self.transform(bgr_img)\n",
    "            \n",
    "        # Normalize image [-1, 1]\n",
    "        bgr_img = (bgr_img - 127.5)/127.5\n",
    "\n",
    "\n",
    "        return bgr_img, kd, ch_1, ch_2, ch_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data from: /home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv\n",
      "vocabulary: ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
      "data has 8325 rows, 24 vocab size (unique).\n",
      "group_encodings: {'none': 204, 'polar': 51, 'nonpolar': 102, 'pos': 85, 'neg': 170}\n",
      "min mutation freq: tensor(0) max mutation freq: tensor(255)\n",
      "some mutation_freq_encoded values: tensor([156, 255, 227,  85, 241, 227,  29, 100, 241,  71])\n",
      "8325\n",
      "config[vocab_size]: 24 , config[block_size]: 242\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "\n",
    "# train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "#                             RandomHorizontalFlip(p=0.25),\n",
    "#                             RandomVerticalFlip(p=0.25)])\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset_BGR(config, train_data_path) #, train_transforms)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch_1: torch.Size([44, 44]) tensor(0.) tensor(1.)\n",
      "ch_2: torch.Size([44, 44]) tensor(0.) tensor(1.)\n",
      "ch_3: torch.Size([44, 44]) tensor(0.) tensor(1.)\n",
      "torch.float32\n",
      "img shape: torch.Size([3, 44, 44]) , kd: tensor([2.4634])\n",
      "tensor(-1.) ,  tensor(1.) ,  tensor(-0.3578) ,  tensor(0.9339)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGeCAYAAADSRtWEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfNElEQVR4nO3de2xUZf7H8c9A2+HWGSiXDpWWRWAlSooRBSe6qEuXioZw6R+uMbusEg04EKBZXZussu4lZf0lXthFJDERjWJNTcDgRhSLDNlsYaHQFJVthGW3NdCymvRMrXYg7fP7Y3/OzxFoO53pPGeG9yv5Jsw5p2e+88wwn5yZ88zxGGOMAABIs2G2GwAAXJ0IIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACtybDfwfb29vTp79qzy8/Pl8XhstwMASJAxRp2dnSoqKtKwYX0c55gh8uc//9lMnTrVeL1eM2/ePHP48OEB/V1ra6uRRFEURWV4tba29vl+PyQfwb311luqrKzUpk2bdOzYMc2ZM0fl5eU6f/58v3+bn58/FC0BANKs3/fzVBztfN+8efNMKBSK3e7p6TFFRUWmurq63791HMd6alMURVHJl+M4fb7fp/wI6MKFC2poaFBZWVls2bBhw1RWVqb6+vpLto9Go4pEInEFAMh+KQ+gL774Qj09PSosLIxbXlhYqLa2tku2r66ult/vj1VxcXGqWwIAuJD107CrqqrkOE6sWltbbbcEAEiDlJ+GPWHCBA0fPlzt7e1xy9vb2xUIBC7Z3uv1yuv1proNAIDLpfwIKC8vT3PnzlVdXV1sWW9vr+rq6hQMBlN9dwCADDUkE1ErKyu1cuVK3XzzzZo3b56ef/55dXV16cEHHxyKuwMAZKAhCaD77rtP//nPf/TUU0+pra1NN954o/bu3XvJiQkAgKuXxxhjbDfxXZFIRH6/33YbAIAkOY4jn893xfXWz4IDAFydCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYkWO7ASDTGGNstwC4WiQSkd/v73e7lB8B/eY3v5HH44mrWbNmpfpuAAAZbkiOgG644QZ9+OGH/38nORxoAQDiDUky5OTkKBAIDMWuAQBZYkhOQvjss89UVFSka6+9Vg888IBaWlquuG00GlUkEokrAED2S3kAzZ8/Xzt27NDevXu1bds2nTlzRj/60Y/U2dl52e2rq6vl9/tjVVxcnOqWAAAu5DFDfEpPR0eHpk6dqmeffVarVq26ZH00GlU0Go3djkQihBBcjbPggL59exac4zjy+XxX3G7Izw4YO3asfvjDH+rUqVOXXe/1euX1eoe6DQCAywx5AH311Vc6ffq0fvaznyX0d/0lJ4DkeTyeIb+Pq+mIkfFMTMq/A/rlL3+pcDisf/3rX/rb3/6m5cuXa/jw4br//vtTfVcAgAyW8iOgzz//XPfff7++/PJLTZw4UbfffrsOHTqkiRMnpvquAAAZLOUBVFNTk+pdAgCyED9GCgCwggACAFhBAAEArCCAAABWEEAAACtce52EgVzMKBnZNJkLGKxs+X/glgmg2TKe6cIREADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArXDsPiAvSpU865lBI6Zkj4Zb5IMkayOPor490Pa/Zwi3Pa39S8bynYx8DwREQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFa6diNrXBekGMgGqv4lU6dhHpkzwTMWEsoE81lQ8J/1Jx3OSjkl6qXh9puJ+0vH/KBXcMom0vz7S0Wc6/h+lCkdAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKxw7Tygq+WCdG6Yv+CWeVVu4Ya5RAMZTzfMKcmUuW4D4Zbn1Q1ce0G6gwcPasmSJSoqKpLH49Hu3bsvaeqpp57S5MmTNXLkSJWVlemzzz5LulEAQHZJOIC6uro0Z84cbd269bLrn3nmGW3ZskUvvfSSDh8+rNGjR6u8vFzd3d1JNwsAyB4JfwS3ePFiLV68+LLrjDF6/vnn9etf/1pLly6VJL322msqLCzU7t279dOf/jS5bgEAWSOlJyGcOXNGbW1tKisriy3z+/2aP3++6uvrL/s30WhUkUgkrgAA2S+lAdTW1iZJKiwsjFteWFgYW/d91dXV8vv9sSouLk5lSwAAl7J+GnZVVZUcx4lVa2ur7ZYAAGmQ0gAKBAKSpPb29rjl7e3tsXXf5/V65fP54goAkP1SOg9o2rRpCgQCqqur04033ihJikQiOnz4sNasWZPQvpK9HlAquGUOxFBL1zn/bpgD4ZbH4ZbXVrbMJXLDtabcIpPelxIOoK+++kqnTp2K3T5z5owaGxtVUFCgkpISbdiwQb///e81c+ZMTZs2TU8++aSKioq0bNmyVPYNAMhwCQfQ0aNHddddd8VuV1ZWSpJWrlypHTt26PHHH1dXV5ceeeQRdXR06Pbbb9fevXs1YsSI1HUNAMh4HuOy48pIJNLnx29S+n46Jtn7GIhM+YkRl71Msl6mfASXio9v07EPt7xnpIMbXjvfvo/395Nq1s+CAwBcnQggAIAVBBAAwAoCCABgBQEEALAiay9I54aJfm45064/6Tq7J1POVErHWYNX05mJmTQxsi+Z8pxlyutC4ggIAGAJAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBWunQfU3y9iJ8stv0KdLRc3S8dFxdK1j/5cTRc3649bHgfPSWbiCAgAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAK105ETfaCdKnghgtppeIibemQrj6z5UJvbukzUybu9icdFztMhWya7JqK8eIICABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVrp0HlOwF6dJxvr1b5mEkOzfGLRfOGwg3zHlKBbfMB0m2j3Q9H5nyvLvheU3XWPX1WCORyIDewxM+Ajp48KCWLFmioqIieTwe7d69O279L37xC3k8nri6++67E70bAECWSziAurq6NGfOHG3duvWK29x99906d+5crN58882kmgQAZJ+EP4JbvHixFi9e3Oc2Xq9XgUBg0E0BALLfkJyEcODAAU2aNEnXXXed1qxZoy+//PKK20ajUUUikbgCAGS/lAfQ3Xffrddee011dXX64x//qHA4rMWLF6unp+ey21dXV8vv98equLg41S0BAFzIY5I4bcPj8WjXrl1atmzZFbf55z//qenTp+vDDz/UwoULL1kfjUYVjUZjtyORSEpCKB1nXLnlrK5s+YXogeAXi90lXWdQJvu8u+XX2tPBTWfB9XdVgyGfB3TttddqwoQJOnXq1GXXe71e+Xy+uAIAZL8hD6DPP/9cX375pSZPnjzUdwUAyCAJnwX31VdfxR3NnDlzRo2NjSooKFBBQYGefvppVVRUKBAI6PTp03r88cc1Y8YMlZeXJ3Q/brggXaZMZk3HfWTLR1/p+ogkHR8ZpcLVMsEzUx7nQKTjq4FU9DEQCQfQ0aNHddddd8VuV1ZWSpJWrlypbdu2qampSa+++qo6OjpUVFSkRYsW6Xe/+528Xm/SzQIAskdSJyEMhYF+eYX0cssR0NVyZJEpfQ6EG95i3PDFfKpk0hGQ9ZMQAAC4HAIIAGAFAQQAsIIAAgBYQQABAKzggnR9uFrOZBpIn+kYz2z5KZSrSSrOXEzXPvrjltdOso81FT+PlOw+huyCdAAApAIBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFa6dB5Tsr2G7Yd5Aui5X7Jb5C/3JlD5TIVMea7Kv4Uz5lelsmq/Un0x57UkcAQEALCGAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFjh2omofV3MKF0TrdIx+a0/6Zjol64+0/G8XU0XtcuU10Y6pGtCbLJS0adbHmsq+uAICABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVrp0H1JdsOpfeDdI1Fpky5pnSZzowFv+PsUi9hI6Aqqurdcsttyg/P1+TJk3SsmXL1NzcHLdNd3e3QqGQxo8frzFjxqiiokLt7e0pbRoAkPkSCqBwOKxQKKRDhw5p3759unjxohYtWqSurq7YNhs3btSePXtUW1urcDiss2fPasWKFSlvHACQ4UwSzp8/bySZcDhsjDGmo6PD5Obmmtra2tg2J0+eNJJMfX39gPbpOI6RRFEURWV4OY7T5/t9UichOI4jSSooKJAkNTQ06OLFiyorK4ttM2vWLJWUlKi+vv6y+4hGo4pEInEFAMh+gw6g3t5ebdiwQbfddptmz54tSWpra1NeXp7Gjh0bt21hYaHa2touu5/q6mr5/f5YFRcXD7YlAEAGGXQAhUIhffzxx6qpqUmqgaqqKjmOE6vW1tak9gcAyAyDOg177dq1evfdd3Xw4EFNmTIltjwQCOjChQvq6OiIOwpqb29XIBC47L68Xq+8Xu9g2gAAZLCEjoCMMVq7dq127dql/fv3a9q0aXHr586dq9zcXNXV1cWWNTc3q6WlRcFgMDUdAwCyQkJHQKFQSDt37tQ777yj/Pz82Pc6fr9fI0eOlN/v16pVq1RZWamCggL5fD6tW7dOwWBQt95665A8AABAhkrktGtd4VS7V155JbbNN998Yx599FEzbtw4M2rUKLN8+XJz7ty5Ad8Hp2FTFEVlR/V3Grbn/4LFNSKRSJ+X4wYAZAbHceTz+a64nh8jBQBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGBFQgFUXV2tW265Rfn5+Zo0aZKWLVum5ubmuG3uvPNOeTyeuFq9enVKmwYAZL6EAigcDisUCunQoUPat2+fLl68qEWLFqmrqytuu4cffljnzp2L1TPPPJPSpgEAmS8nkY337t0bd3vHjh2aNGmSGhoatGDBgtjyUaNGKRAIpKZDAEBWSuo7IMdxJEkFBQVxy9944w1NmDBBs2fPVlVVlb7++usr7iMajSoSicQVAOAqYAapp6fH3Hvvvea2226LW759+3azd+9e09TUZF5//XVzzTXXmOXLl19xP5s2bTKSKIqiqCwrx3H6zJFBB9Dq1avN1KlTTWtra5/b1dXVGUnm1KlTl13f3d1tHMeJVWtrq/VBoyiKopKv/gIooe+AvrV27Vq9++67OnjwoKZMmdLntvPnz5cknTp1StOnT79kvdfrldfrHUwbAIAMllAAGWO0bt067dq1SwcOHNC0adP6/ZvGxkZJ0uTJkwfVIAAgOyUUQKFQSDt37tQ777yj/Px8tbW1SZL8fr9Gjhyp06dPa+fOnbrnnns0fvx4NTU1aePGjVqwYIFKS0uH5AEAADJUIt/76Aqf873yyivGGGNaWlrMggULTEFBgfF6vWbGjBnmscce6/dzwO9yHMf655YURVFU8tXfe7/n/4LFNSKRiPx+v+02AABJchxHPp/viuv5LTgAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAViQUQNu2bVNpaal8Pp98Pp+CwaDee++92Pru7m6FQiGNHz9eY8aMUUVFhdrb21PeNAAg8yUUQFOmTNHmzZvV0NCgo0eP6sc//rGWLl2qTz75RJK0ceNG7dmzR7W1tQqHwzp79qxWrFgxJI0DADKcSdK4cePMyy+/bDo6Okxubq6pra2NrTt58qSRZOrr6we8P8dxjCSKoigqw8txnD7f7wf9HVBPT49qamrU1dWlYDCohoYGXbx4UWVlZbFtZs2apZKSEtXX119xP9FoVJFIJK4AANkv4QA6ceKExowZI6/Xq9WrV2vXrl26/vrr1dbWpry8PI0dOzZu+8LCQrW1tV1xf9XV1fL7/bEqLi5O+EEAADJPwgF03XXXqbGxUYcPH9aaNWu0cuVKffrpp4NuoKqqSo7jxKq1tXXQ+wIAZI6cRP8gLy9PM2bMkCTNnTtXR44c0QsvvKD77rtPFy5cUEdHR9xRUHt7uwKBwBX35/V65fV6E+8cAJDRkp4H1Nvbq2g0qrlz5yo3N1d1dXWxdc3NzWppaVEwGEz2bgAAWSahI6CqqiotXrxYJSUl6uzs1M6dO3XgwAG9//778vv9WrVqlSorK1VQUCCfz6d169YpGAzq1ltvHar+AQAZKqEAOn/+vH7+85/r3Llz8vv9Ki0t1fvvv6+f/OQnkqTnnntOw4YNU0VFhaLRqMrLy/Xiiy8OSeMAgMzmMcYY2018VyQSkd/vt90GACBJjuPI5/NdcT2/BQcAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADAioQCaNu2bSotLZXP55PP51MwGNR7770XW3/nnXfK4/HE1erVq1PeNAAg8+UksvGUKVO0efNmzZw5U8YYvfrqq1q6dKmOHz+uG264QZL08MMP67e//W3sb0aNGpXajgEAWSGhAFqyZEnc7T/84Q/atm2bDh06FAugUaNGKRAIpK5DAEBWGvR3QD09PaqpqVFXV5eCwWBs+RtvvKEJEyZo9uzZqqqq0tdff93nfqLRqCKRSFwBAK4CJkFNTU1m9OjRZvjw4cbv95u//OUvsXXbt283e/fuNU1NTeb1118311xzjVm+fHmf+9u0aZORRFEURWVZOY7T5/u/xxhjlIALFy6opaVFjuPo7bff1ssvv6xwOKzrr7/+km3379+vhQsX6tSpU5o+ffpl9xeNRhWNRmO3I5GIiouLE2kJAOBCjuPI5/NdcX3CAfR9ZWVlmj59urZv337Juq6uLo0ZM0Z79+5VeXn5gPYXiUTk9/uTaQkA4AL9BVDS84B6e3vjjmC+q7GxUZI0efLkZO8GAJBlEjoLrqqqSosXL1ZJSYk6Ozu1c+dOHThwQO+//75Onz6tnTt36p577tH48ePV1NSkjRs3asGCBSotLR2q/gEAmSqRExAeeughM3XqVJOXl2cmTpxoFi5caD744ANjjDEtLS1mwYIFpqCgwHi9XjNjxgzz2GOP9fsl1Pc5jmP9izOKoigq+Ur5SQhDje+AACA7DPl3QAAADAYBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFUkF0ObNm+XxeLRhw4bYsu7uboVCIY0fP15jxoxRRUWF2tvbk+0TAJBlBh1AR44c0fbt21VaWhq3fOPGjdqzZ49qa2sVDod19uxZrVixIulGAQBZxgxCZ2enmTlzptm3b5+54447zPr1640xxnR0dJjc3FxTW1sb2/bkyZNGkqmvrx/Qvh3HMZIoiqKoDC/Hcfp8vx/UEVAoFNK9996rsrKyuOUNDQ26ePFi3PJZs2appKRE9fX1l91XNBpVJBKJKwBA9stJ9A9qamp07NgxHTly5JJ1bW1tysvL09ixY+OWFxYWqq2t7bL7q66u1tNPP51oGwCADJfQEVBra6vWr1+vN954QyNGjEhJA1VVVXIcJ1atra0p2S8AwN0SCqCGhgadP39eN910k3JycpSTk6NwOKwtW7YoJydHhYWFunDhgjo6OuL+rr29XYFA4LL79Hq98vl8cQUAyH4JfQS3cOFCnThxIm7Zgw8+qFmzZulXv/qViouLlZubq7q6OlVUVEiSmpub1dLSomAwmLquAQAZL6EAys/P1+zZs+OWjR49WuPHj48tX7VqlSorK1VQUCCfz6d169YpGAzq1ltvTV3XAICMl/BJCP157rnnNGzYMFVUVCgajaq8vFwvvvhiqu8GAJDhPMYYY7uJ74pEIvL7/bbbAAAkyXGcPr/X57fgAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYIXrAsgYY7sFAEAK9Pd+7roA6uzstN0CACAF+ns/9xiXHXL09vbq7Nmzys/Pl8fjkSRFIhEVFxertbVVPp/PcoeZj/FMLcYztRjP1LIxnsYYdXZ2qqioSMOGXfk4Jyct3SRg2LBhmjJlymXX+Xw+XpApxHimFuOZWoxnaqV7PP1+f7/buO4jOADA1YEAAgBYkREB5PV6tWnTJnm9XtutZAXGM7UYz9RiPFPLzePpupMQAABXh4w4AgIAZB8CCABgBQEEALCCAAIAWEEAAQCscH0Abd26VT/4wQ80YsQIzZ8/X3//+99tt5QRDh48qCVLlqioqEgej0e7d++OW2+M0VNPPaXJkydr5MiRKisr02effWan2QxQXV2tW265Rfn5+Zo0aZKWLVum5ubmuG26u7sVCoU0fvx4jRkzRhUVFWpvb7fUsbtt27ZNpaWlsdn5wWBQ7733Xmw9Yzl4mzdvlsfj0YYNG2LL3Dqerg6gt956S5WVldq0aZOOHTumOXPmqLy8XOfPn7fdmut1dXVpzpw52rp162XXP/PMM9qyZYteeuklHT58WKNHj1Z5ebm6u7vT3GlmCIfDCoVCOnTokPbt26eLFy9q0aJF6urqim2zceNG7dmzR7W1tQqHwzp79qxWrFhhsWv3mjJlijZv3qyGhgYdPXpUP/7xj7V06VJ98sknkhjLwTpy5Ii2b9+u0tLSuOWuHU/jYvPmzTOhUCh2u6enxxQVFZnq6mqLXWUeSWbXrl2x2729vSYQCJj/+Z//iS3r6OgwXq/XvPnmmxY6zDznz583kkw4HDbG/Hf8cnNzTW1tbWybkydPGkmmvr7eVpsZZdy4cebll19mLAeps7PTzJw50+zbt8/ccccdZv369cYYd782XXsEdOHCBTU0NKisrCy2bNiwYSorK1N9fb3FzjLfmTNn1NbWFje2fr9f8+fPZ2wHyHEcSVJBQYEkqaGhQRcvXowb01mzZqmkpIQx7UdPT49qamrU1dWlYDDIWA5SKBTSvffeGzdukrtfm677NexvffHFF+rp6VFhYWHc8sLCQv3jH/+w1FV2aGtrk6TLju2363Blvb292rBhg2677TbNnj1b0n/HNC8vT2PHjo3bljG9shMnTigYDKq7u1tjxozRrl27dP3116uxsZGxTFBNTY2OHTumI0eOXLLOza9N1wYQ4FahUEgff/yx/vrXv9puJaNdd911amxslOM4evvtt7Vy5UqFw2HbbWWc1tZWrV+/Xvv27dOIESNst5MQ134EN2HCBA0fPvySMzXa29sVCAQsdZUdvh0/xjZxa9eu1bvvvquPPvoo7rpVgUBAFy5cUEdHR9z2jOmV5eXlacaMGZo7d66qq6s1Z84cvfDCC4xlghoaGnT+/HnddNNNysnJUU5OjsLhsLZs2aKcnBwVFha6djxdG0B5eXmaO3eu6urqYst6e3tVV1enYDBosbPMN23aNAUCgbixjUQiOnz4MGN7BcYYrV27Vrt27dL+/fs1bdq0uPVz585Vbm5u3Jg2NzerpaWFMR2g3t5eRaNRxjJBCxcu1IkTJ9TY2Birm2++WQ888EDs364dT6unQPSjpqbGeL1es2PHDvPpp5+aRx55xIwdO9a0tbXZbs31Ojs7zfHjx83x48eNJPPss8+a48ePm3//+9/GGGM2b95sxo4da9555x3T1NRkli5daqZNm2a++eYby52705o1a4zf7zcHDhww586di9XXX38d22b16tWmpKTE7N+/3xw9etQEg0ETDAYtdu1eTzzxhAmHw+bMmTOmqanJPPHEE8bj8ZgPPvjAGMNYJuu7Z8EZ497xdHUAGWPMn/70J1NSUmLy8vLMvHnzzKFDh2y3lBE++ugjI+mSWrlypTHmv6diP/nkk6awsNB4vV6zcOFC09zcbLdpF7vcWEoyr7zySmybb775xjz66KNm3LhxZtSoUWb58uXm3Llz9pp2sYceeshMnTrV5OXlmYkTJ5qFCxfGwscYxjJZ3w8gt44n1wMCAFjh2u+AAADZjQACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArPhfo8ATH5a2x3sAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, kd, ch_1, ch_2, ch_3 = train_dataset.__getitem__(105)\n",
    "print(img.dtype)\n",
    "# change the order of the channels to be (H, W) instead of (C, H, W)\n",
    "rgb_img = img.permute(1, 2, 0)\n",
    "\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "# plt.imshow(rgb_img) #, cmap='gray')\n",
    "\n",
    "plt.imshow(ch_3, cmap='gray')\n",
    "\n",
    "print(torch.min(rgb_img), ', ', torch.max(rgb_img), ', ', torch.mean(rgb_img), ', ', torch.std(rgb_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Examine other data that may be added as input channels to Vision Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the amino acids into their usual groups\n",
    "# polar, nonpolar, positively charged, negatively charged, or none (i.e. CLS, SEP, PAD)\n",
    "#\n",
    "# 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "# 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "         'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "print('\\nvocabulary:', chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "group_encodings = { 'none'    : int('00101000', base=2), \n",
    "                    'polar'   : int('00110011', base=2),\n",
    "                    'nonpolar': int('11001100', base=2), \n",
    "                    'pos'     : int('01010101', base=2),\n",
    "                    'neg'     : int('10101010', base=2)} \n",
    "\n",
    "print('group_encodings:', group_encodings)\n",
    "\n",
    "# maps amino acid to group                     \n",
    "s_to_grp = {ch:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "# maps encoded residue to group\n",
    "i_to_grp = {stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "print('\\ns_to_grp:', s_to_grp)\n",
    "print('\\ni_to_grp:', i_to_grp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _bin(x):\n",
    "    return format(x, '08b')\n",
    "\n",
    "def _encode_channel(x, shape=(48,48)):\n",
    "    d = ''.join([_bin(x[i]) for i in x.numpy()])\n",
    "    # turn d into a list of integers, one for each bit\n",
    "    d = [int(x) for x in d]    \n",
    "    t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "    t = t.reshape(shape)\n",
    "    # t = t.unsqueeze(0) # add channel dimension\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a heat-map for the variability of each position in the sequence?\n",
    "# That somehow changes with each sequence?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
