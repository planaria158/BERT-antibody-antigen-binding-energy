{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mark/dev/aAlphaBio-Homework/notebooks', '/home/mark/anaconda3/envs/avm-dvm/lib/python39.zip', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/lib-dynload', '', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages', '/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/']\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append('/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/') \n",
    "print(sys.path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 145, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_patches = 144\n",
    "dim = 128\n",
    "a = torch.randn(1, num_patches + 1, dim)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_size': 242, 'mask_prob': 0.0, 'image_shape': [44, 44], 'patch_dim': 4, 'num_heads': 8, 'num_layers': 6, 'dim': 128, 'dropout': 0.3, 'image_channels': 3, 'accelerator': 'gpu', 'devices': 2, 'batch_size': 512, 'num_workers': 10, 'grad_norm_clip': 1.0, 'num_epochs': 1000, 'log_dir': './lightning_logs/vit_model/cleaned-3/BGR/', 'train_data_path': '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv', 'test_data_path': '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_val_set.csv', 'checkpoint_name': 'None', 'learning_rate': 0.0001, 'lr_gamma': 0.999, 'betas': [0.9, 0.95], 'checkpoint_every_n_train_steps': 100, 'save_top_k': 4, 'monitor': 'val_loss', 'mode': 'min', 'log_every_nsteps': 10, 'inference_results_folder': './inference_results/vit/cleaned-3/', 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "# Read the config\n",
    "config_path = '../config/vit_params.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.inference = inference\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "        affinity = self.df.loc[idx, 'Kd'] if self.inference == False else 0.0\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert affinity >= 0.0, 'affinity cannot be negative'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) <= self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # print('chunk length:', len(chunk), ', chunk:', chunk)\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        first_aa = 1 # first aa position in the sequence (after CLS)\n",
    "        last_aa = dix.shape[0] # last aa position in the sequence\n",
    "        # print('first_aa:', first_aa, ', last_aa:', last_aa)\n",
    "        if dix.shape[0] < self.config['block_size']:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.config['block_size'] - len(dix)), dtype=torch.long)))\n",
    "\n",
    "        mask = None\n",
    "        if self.config['mask_prob'] > 0:\n",
    "            # dix looks like: [[CLS], x1, x2, x3, ..., xN, [PAD], [PAD], ..., [PAD]]\n",
    "            # Never mask CLS or PAD tokens\n",
    "\n",
    "            # get number of tokens to mask\n",
    "            n_pred = max(1, int(round((last_aa - first_aa)*self.config['mask_prob'])))\n",
    "\n",
    "            # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "            masked_idx = torch.randperm(last_aa-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "            # ... and overwrite them with MASK token in the data\n",
    "            dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, torch.tensor([affinity], dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets.scFv_dataset import scFv_Dataset\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Simple wrapper Dataset to turn output from the scFv dataset\n",
    "# into a B&W image for use in a CNN model\n",
    "#--------------------------------------------------------\n",
    "class CNN_Dataset_BGR(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, transform=None, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        self.img_shape = config['image_shape']\n",
    "        self.transform = transform\n",
    "         \n",
    "        chars = self.scFv_dataset.chars\n",
    "        groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "                'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "        \n",
    "        # for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "        group_encodings = { 'none'    : int('11001100', base=2), \n",
    "                            'polar'   : int('00110011', base=2),\n",
    "                            'nonpolar': int('01100110', base=2), \n",
    "                            'pos'     : int('01010101', base=2),\n",
    "                            'neg'     : int('10101010', base=2)} \n",
    "        \n",
    "        print('group_encodings:', group_encodings)\n",
    "\n",
    "        # map encoded sequence to groups\n",
    "        self.i_to_grp = {self.scFv_dataset.stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "        print('i_to_grp:', self.i_to_grp)\n",
    "\n",
    "        # The relative mutation frequence for each amino acid position in the scFv sequences over the entire clean_3 dataset\n",
    "        # This fixed-array is 241 elements long.\n",
    "        self.rel_mutation_freq = torch.tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7778,\n",
    "                                                0.9444, 0.9444, 1.0000, 1.0000, 1.0000, 0.9444, 1.0000, 1.0000, 0.8889,\n",
    "                                                0.5556, 0.5000, 0.2222, 0.3333, 0.2778, 0.6111, 0.4444, 0.5556, 1.0000,\n",
    "                                                0.8333, 0.8333, 0.8333, 0.9444, 0.7778, 0.8333, 0.6111, 1.0000, 0.8889,\n",
    "                                                0.3333, 0.9444, 0.8889, 0.1111, 0.3889, 0.9444, 0.2778, 0.9444, 0.8333,\n",
    "                                                0.5000, 1.0000, 1.0000, 1.0000, 0.9444, 0.6111, 0.6111, 0.7778, 0.2778,\n",
    "                                                0.8889, 0.3889, 0.9444, 1.0000, 0.3889, 0.9444, 1.0000, 0.9444, 0.2222,\n",
    "                                                0.7778, 0.5556, 0.8889, 0.2222, 0.7778, 0.6111, 0.6667, 0.8333, 0.8333,\n",
    "                                                1.0000, 1.0000, 0.8889, 0.8333, 0.8333, 0.9444, 0.7222, 0.9444, 0.9444,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]) #, 0.0000, 0.0000,\n",
    "                                                #0.0000, 0.0000, 0.0000])\n",
    "        \n",
    "        print('len(rel_mutation_freq):', len(self.rel_mutation_freq))\n",
    "        \n",
    "        self.mutation_freq_encoded = self.rel_mutation_freq * 255\n",
    "        self.mutation_freq_encoded = torch.floor(self.mutation_freq_encoded).to(torch.long)\n",
    "        print(torch.min(self.mutation_freq_encoded), torch.max(self.mutation_freq_encoded))\n",
    "        print( self.mutation_freq_encoded[0:10])\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _encode_channel(self, x, shape):\n",
    "        d = ''.join([self._bin(val) for val in x])\n",
    "        # turn d into a list of integers, one for each bit\n",
    "        d = [int(x) for x in d]    \n",
    "        t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for shape matrix\n",
    "        t = t.reshape(shape)\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, Kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "\n",
    "        # 50% of the time flip the sequences back-to-front\n",
    "        flip = False\n",
    "        if random.random() > 0.5:\n",
    "            flip = True\n",
    "\n",
    "        dix = torch.flip(dix, [0]) if flip else dix\n",
    "\n",
    "        # The residue encoding channel\n",
    "        ch_1 = self._encode_channel(dix, self.img_shape)\n",
    "\n",
    "        # The residue group encoding channel\n",
    "        dix_grp = torch.tensor([self.i_to_grp[i] for i in dix.numpy().tolist()], dtype=torch.long)\n",
    "        dix_grp = torch.flip(dix_grp, [0]) if flip else dix_grp\n",
    "        ch_2 = self._encode_channel(dix_grp, self.img_shape)\n",
    "\n",
    "        # The mutation frequency channel; anything not an amino acid gets a zero.\n",
    "        ch3_in = torch.zeros_like(dix)\n",
    "        # First aa is always position 1 (0 is a CLS token)\n",
    "        ch3_in[1:len(self.mutation_freq_encoded)+1] = self.mutation_freq_encoded\n",
    "        ch3_in = torch.flip(ch3_in, [0]) if flip else ch3_in\n",
    "        ch_3 = self._encode_channel(ch3_in, self.img_shape)\n",
    "\n",
    "        # stack the 3 channels into an bgr image\n",
    "        bgr_img = torch.stack((ch_1, ch_2, ch_3), dim=0) * 255\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(bgr_img)\n",
    "            # Normalize image [-1, 1]\n",
    "            bgr_img = (bgr_img - 127.5)/127.5\n",
    "\n",
    "\n",
    "        return bgr_img, kd, ch_1, ch_2, ch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data from: /home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv\n",
      "vocabulary: ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
      "data has 8325 rows, 24 vocab size (unique).\n",
      "group_encodings: {'none': 204, 'polar': 51, 'nonpolar': 102, 'pos': 85, 'neg': 170}\n",
      "i_to_grp: {0: 204, 1: 102, 2: 102, 3: 170, 4: 170, 5: 102, 6: 102, 7: 85, 8: 102, 9: 85, 10: 102, 11: 102, 12: 170, 13: 102, 14: 170, 15: 85, 16: 51, 17: 51, 18: 102, 19: 102, 20: 51, 21: 204, 22: 204, 23: 204}\n",
      "len(rel_mutation_freq): 241\n",
      "tensor(0) tensor(255)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "8325\n",
      "config[vocab_size]: 24 , config[block_size]: 242\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "\n",
    "train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "                            RandomHorizontalFlip(p=0.25),\n",
    "                            RandomVerticalFlip(p=0.25)])\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset_BGR(config, train_data_path, train_transforms)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "img shape: torch.Size([3, 44, 44]) , kd: tensor([2.4634])\n",
      "tensor(-1.) ,  tensor(1.) ,  tensor(-0.3678) ,  tensor(0.9300)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, kd, ch_1, ch_2, ch_3 = train_dataset.__getitem__(105)\n",
    "print(img.dtype)\n",
    "# change the order of the channels to be (H, W) instead of (C, H, W)\n",
    "rgb_img = img.permute(1, 2, 0)\n",
    "\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "# plt.imshow(rgb_img) #, cmap='gray')\n",
    "\n",
    "plt.imshow(ch_2, cmap='gray')\n",
    "\n",
    "print(torch.min(rgb_img), ', ', torch.max(rgb_img), ', ', torch.mean(rgb_img), ', ', torch.std(rgb_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Examine other data that may be added as input channels to Vision Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the amino acids into their usual groups\n",
    "# polar, nonpolar, positively charged, negatively charged, or none (i.e. CLS, SEP, PAD)\n",
    "#\n",
    "# 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "# 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "         'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "print('\\nvocabulary:', chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "group_encodings = { 'none'    : int('00101000', base=2), \n",
    "                    'polar'   : int('00110011', base=2),\n",
    "                    'nonpolar': int('11001100', base=2), \n",
    "                    'pos'     : int('01010101', base=2),\n",
    "                    'neg'     : int('10101010', base=2)} \n",
    "\n",
    "print('group_encodings:', group_encodings)\n",
    "\n",
    "# maps amino acid to group                     \n",
    "s_to_grp = {ch:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "# maps encoded residue to group\n",
    "i_to_grp = {stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "print('\\ns_to_grp:', s_to_grp)\n",
    "print('\\ni_to_grp:', i_to_grp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _bin(x):\n",
    "    return format(x, '08b')\n",
    "\n",
    "def _encode_channel(x, shape=(48,48)):\n",
    "    d = ''.join([_bin(x[i]) for i in x.numpy()])\n",
    "    # turn d into a list of integers, one for each bit\n",
    "    d = [int(x) for x in d]    \n",
    "    t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "    t = t.reshape(shape)\n",
    "    # t = t.unsqueeze(0) # add channel dimension\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a heat-map for the variability of each position in the sequence?\n",
    "# That somehow changes with each sequence?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
