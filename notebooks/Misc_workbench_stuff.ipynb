{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/mark/dev/aAlphaBio-Homework/notebooks', '/home/mark/anaconda3/envs/avm-dvm/lib/python39.zip', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/lib-dynload', '', '/home/mark/anaconda3/envs/avm-dvm/lib/python3.9/site-packages', '/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/']\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "sys.path.append('/Users/markthompson/Documents/dev/a-alphaBio-homework/datasets/') \n",
    "print(sys.path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 145, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "num_patches = 144\n",
    "dim = 128\n",
    "a = torch.randn(1, num_patches + 1, dim)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_size': 242, 'mask_prob': 0.0, 'image_shape': [44, 44], 'patch_dim': 4, 'num_heads': 8, 'num_layers': 6, 'dim': 128, 'dropout': 0.3, 'image_channels': 3, 'accelerator': 'gpu', 'devices': 2, 'batch_size': 512, 'num_workers': 10, 'grad_norm_clip': 1.0, 'num_epochs': 1000, 'log_dir': './lightning_logs/vit_model/cleaned-3/BGR/', 'train_data_path': '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv', 'test_data_path': '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_val_set.csv', 'checkpoint_name': 'None', 'learning_rate': 0.0001, 'lr_gamma': 0.999, 'betas': [0.9, 0.95], 'checkpoint_every_n_train_steps': 100, 'save_top_k': 4, 'monitor': 'val_loss', 'mode': 'min', 'log_every_nsteps': 10, 'inference_results_folder': './inference_results/vit/cleaned-3/', 'seed': 3407}\n"
     ]
    }
   ],
   "source": [
    "# Read the config\n",
    "config_path = '../config/vit_params.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "config = config['model_params']\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Code fragments taken from:\n",
    "# * https://github.com/barneyhill/minBERT\n",
    "# * https://github.com/karpathy/minGPT\n",
    "\n",
    "# protein sequence data taken from:\n",
    "# * https://www.nature.com/articles/s41467-023-39022-2\n",
    "# * https://zenodo.org/records/7783546\n",
    "#--------------------------------------------------------\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of amino acid sequences and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.inference = inference\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        print('vocabulary:', self.chars)\n",
    "\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] #len(self.data) - self.config['block_size']\n",
    "\n",
    "    \"\"\" Returns data, mask pairs used for Masked Language Model training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "        affinity = self.df.loc[idx, 'Kd'] if self.inference == False else 0.0\n",
    "        assert not math.isnan(affinity), 'affinity is nan'\n",
    "        assert affinity >= 0.0, 'affinity cannot be negative'\n",
    "\n",
    "        # get a randomly located block_size-1 substring from the sequence\n",
    "        # '-1' so we can prepend the CLS token to the start of the encoded string\n",
    "        if len(seq) <= self.config['block_size']-1:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.config['block_size'] - 1))\n",
    "            chunk = seq[start_idx:start_idx + self.config['block_size']-1]\n",
    "\n",
    "        # print('chunk length:', len(chunk), ', chunk:', chunk)\n",
    "\n",
    "        # encode every character to an integer\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        first_aa = 1 # first aa position in the sequence (after CLS)\n",
    "        last_aa = dix.shape[0] # last aa position in the sequence\n",
    "        # print('first_aa:', first_aa, ', last_aa:', last_aa)\n",
    "        if dix.shape[0] < self.config['block_size']:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.config['block_size'] - len(dix)), dtype=torch.long)))\n",
    "\n",
    "        mask = None\n",
    "        if self.config['mask_prob'] > 0:\n",
    "            # dix looks like: [[CLS], x1, x2, x3, ..., xN, [PAD], [PAD], ..., [PAD]]\n",
    "            # Never mask CLS or PAD tokens\n",
    "\n",
    "            # get number of tokens to mask\n",
    "            n_pred = max(1, int(round((last_aa - first_aa)*self.config['mask_prob'])))\n",
    "\n",
    "            # indices of the tokens that will be masked (a random selection of n_pred of the tokens)\n",
    "            masked_idx = torch.randperm(last_aa-1, dtype=torch.long, )[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "            \n",
    "            # ... and overwrite them with MASK token in the data\n",
    "            dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, torch.tensor([affinity], dtype=torch.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets.scFv_dataset import scFv_Dataset\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Simple wrapper Dataset to turn output from the scFv dataset\n",
    "# into a B&W image for use in a CNN model\n",
    "#--------------------------------------------------------\n",
    "class CNN_Dataset_BGR(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, transform=None, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        self.img_shape = config['image_shape']\n",
    "        self.transform = transform\n",
    "         \n",
    "        chars = self.scFv_dataset.chars\n",
    "        groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "                'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "        \n",
    "        # for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "        group_encodings = { 'none'    : int('11001100', base=2), \n",
    "                            'polar'   : int('00110011', base=2),\n",
    "                            'nonpolar': int('01100110', base=2), \n",
    "                            'pos'     : int('01010101', base=2),\n",
    "                            'neg'     : int('10101010', base=2)} \n",
    "        \n",
    "        print('group_encodings:', group_encodings)\n",
    "\n",
    "        # map encoded sequence to groups\n",
    "        self.i_to_grp = {self.scFv_dataset.stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "        print('i_to_grp:', self.i_to_grp)\n",
    "\n",
    "        # The relative mutation frequence for each amino acid position in the scFv sequences over the entire clean_3 dataset\n",
    "        # This fixed-array is 241 elements long.\n",
    "        self.rel_mutation_freq = torch.tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7778,\n",
    "                                                0.9444, 0.9444, 1.0000, 1.0000, 1.0000, 0.9444, 1.0000, 1.0000, 0.8889,\n",
    "                                                0.5556, 0.5000, 0.2222, 0.3333, 0.2778, 0.6111, 0.4444, 0.5556, 1.0000,\n",
    "                                                0.8333, 0.8333, 0.8333, 0.9444, 0.7778, 0.8333, 0.6111, 1.0000, 0.8889,\n",
    "                                                0.3333, 0.9444, 0.8889, 0.1111, 0.3889, 0.9444, 0.2778, 0.9444, 0.8333,\n",
    "                                                0.5000, 1.0000, 1.0000, 1.0000, 0.9444, 0.6111, 0.6111, 0.7778, 0.2778,\n",
    "                                                0.8889, 0.3889, 0.9444, 1.0000, 0.3889, 0.9444, 1.0000, 0.9444, 0.2222,\n",
    "                                                0.7778, 0.5556, 0.8889, 0.2222, 0.7778, 0.6111, 0.6667, 0.8333, 0.8333,\n",
    "                                                1.0000, 1.0000, 0.8889, 0.8333, 0.8333, 0.9444, 0.7222, 0.9444, 0.9444,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]) #, 0.0000, 0.0000,\n",
    "                                                #0.0000, 0.0000, 0.0000])\n",
    "        \n",
    "        print('len(rel_mutation_freq):', len(self.rel_mutation_freq))\n",
    "        \n",
    "        self.mutation_freq_encoded = self.rel_mutation_freq * 255\n",
    "        self.mutation_freq_encoded = torch.floor(self.mutation_freq_encoded).to(torch.long)\n",
    "        print(torch.min(self.mutation_freq_encoded), torch.max(self.mutation_freq_encoded))\n",
    "        print( self.mutation_freq_encoded[0:10])\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _encode_channel(self, x, shape):\n",
    "        d = ''.join([self._bin(val) for val in x])\n",
    "        # turn d into a list of integers, one for each bit\n",
    "        d = [int(x) for x in d]    \n",
    "        t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for shape matrix\n",
    "        t = t.reshape(shape)\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, Kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "\n",
    "        # 50% of the time flip the sequences back-to-front\n",
    "        flip = False\n",
    "        if random.random() > 0.5:\n",
    "            flip = True\n",
    "\n",
    "        dix = torch.flip(dix, [0]) if flip else dix\n",
    "\n",
    "        # The residue encoding channel\n",
    "        ch_1 = self._encode_channel(dix, self.img_shape)\n",
    "\n",
    "        # The residue group encoding channel\n",
    "        dix_grp = torch.tensor([self.i_to_grp[i] for i in dix.numpy().tolist()], dtype=torch.long)\n",
    "        dix_grp = torch.flip(dix_grp, [0]) if flip else dix_grp\n",
    "        ch_2 = self._encode_channel(dix_grp, self.img_shape)\n",
    "\n",
    "        # The mutation frequency channel; anything not an amino acid gets a zero.\n",
    "        ch3_in = torch.zeros_like(dix)\n",
    "        # First aa is always position 1 (0 is a CLS token)\n",
    "        ch3_in[1:len(self.mutation_freq_encoded)+1] = self.mutation_freq_encoded\n",
    "        ch3_in = torch.flip(ch3_in, [0]) if flip else ch3_in\n",
    "        ch_3 = self._encode_channel(ch3_in, self.img_shape)\n",
    "\n",
    "        # stack the 3 channels into an bgr image\n",
    "        bgr_img = torch.stack((ch_1, ch_2, ch_3), dim=0) * 255\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(bgr_img)\n",
    "            # Normalize image [-1, 1]\n",
    "            bgr_img = (bgr_img - 127.5)/127.5\n",
    "\n",
    "\n",
    "        return bgr_img, kd, ch_1, ch_2, ch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data from: /home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv\n",
      "vocabulary: ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
      "data has 8325 rows, 24 vocab size (unique).\n",
      "group_encodings: {'none': 204, 'polar': 51, 'nonpolar': 102, 'pos': 85, 'neg': 170}\n",
      "i_to_grp: {0: 204, 1: 102, 2: 102, 3: 170, 4: 170, 5: 102, 6: 102, 7: 85, 8: 102, 9: 85, 10: 102, 11: 102, 12: 170, 13: 102, 14: 170, 15: 85, 16: 51, 17: 51, 18: 102, 19: 102, 20: 51, 21: 204, 22: 204, 23: 204}\n",
      "len(rel_mutation_freq): 241\n",
      "tensor(0) tensor(255)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "8325\n",
      "config[vocab_size]: 24 , config[block_size]: 242\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "\n",
    "train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "                            RandomHorizontalFlip(p=0.25),\n",
    "                            RandomVerticalFlip(p=0.25)])\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset_BGR(config, train_data_path, train_transforms)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "img shape: torch.Size([3, 44, 44]) , kd: tensor([2.4634])\n",
      "tensor(-1.) ,  tensor(1.) ,  tensor(-0.3667) ,  tensor(0.9304)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, kd, ch_1, ch_2, ch_3 = train_dataset.__getitem__(105)\n",
    "print(img.dtype)\n",
    "# change the order of the channels to be (H, W) instead of (C, H, W)\n",
    "rgb_img = img.permute(1, 2, 0)\n",
    "\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "# plt.imshow(rgb_img) #, cmap='gray')\n",
    "\n",
    "plt.imshow(ch_2, cmap='gray')\n",
    "\n",
    "print(torch.min(rgb_img), ', ', torch.max(rgb_img), ', ', torch.mean(rgb_img), ', ', torch.std(rgb_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Examine other data that may be added as input channels to Vision Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the amino acids into their usual groups\n",
    "# polar, nonpolar, positively charged, negatively charged, or none (i.e. CLS, SEP, PAD)\n",
    "#\n",
    "# 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "# 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "         'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "print('\\nvocabulary:', chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "group_encodings = { 'none'    : int('00101000', base=2), \n",
    "                    'polar'   : int('00110011', base=2),\n",
    "                    'nonpolar': int('11001100', base=2), \n",
    "                    'pos'     : int('01010101', base=2),\n",
    "                    'neg'     : int('10101010', base=2)} \n",
    "\n",
    "print('group_encodings:', group_encodings)\n",
    "\n",
    "# maps amino acid to group                     \n",
    "s_to_grp = {ch:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "# maps encoded residue to group\n",
    "i_to_grp = {stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "print('\\ns_to_grp:', s_to_grp)\n",
    "print('\\ni_to_grp:', i_to_grp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _bin(x):\n",
    "    return format(x, '08b')\n",
    "\n",
    "def _encode_channel(x, shape=(48,48)):\n",
    "    d = ''.join([_bin(x[i]) for i in x.numpy()])\n",
    "    # turn d into a list of integers, one for each bit\n",
    "    d = [int(x) for x in d]    \n",
    "    t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "    t = t.reshape(shape)\n",
    "    # t = t.unsqueeze(0) # add channel dimension\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a heat-map for the variability of each position in the sequence?\n",
    "# That somehow changes with each sequence?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
