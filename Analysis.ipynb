{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### Mark Thompson\n",
    "### Started April 29, 2024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Some plotting functions\n",
    "#\n",
    "def plot_preds_hist(preds_file_path):\n",
    "    preds = pk.load(open(preds_file_path, 'rb'))\n",
    "    print('len(preds):', len(preds))\n",
    "    preds = [p[0] for p in preds]\n",
    "    print('preds[0:10]:', preds[0:10])\n",
    "\n",
    "    # Histogram of predicted values\n",
    "    plt.hist(preds, bins=100)\n",
    "    plt.xlabel('pred Kd (nm)')\n",
    "    plt.ylabel('count')\n",
    "    plt.title('Distribution of pred values set')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred_vs_true(preds_file_path, true_file_path, xlim=(0,5), ylim=(0,5)):\n",
    "    preds = pk.load(open(preds_file_path, 'rb'))\n",
    "    y = pk.load(open(true_file_path, 'rb'))\n",
    "    print('len(preds):', len(preds), ', len(y):', len(y))\n",
    "    preds = [p[0] for p in preds]\n",
    "    y = [a[0] for a in y]\n",
    "\n",
    "    # scatter plot of true vs pred\n",
    "    plt.scatter(y, preds, c =\"blue\")\n",
    "    plt.xlabel('experimental Kd (nm)')\n",
    "    plt.ylabel('predicted Kd (nm)')\n",
    "    plt.title('true vs predicted Kd on validation set')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Holdout dataset and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The holdout data\n",
    "data_file = './data/alphaseq_data_hold_out.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('holdout dataframe has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df['sequence_a'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "\n",
    "# tform_mlp version 1 predictions\n",
    "data_file = './inference_results/tform_mlp_model/cleaned-4-data/preds_tform_mlp_1715104590.5575511.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('holdout predictions has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "preds = df['pred_Kd'].values\n",
    "\n",
    "# tform_mlp_v2 version 2 predictions\n",
    "data_file = './inference_results/tform_mlp_model_v2/addendum/cleaned-4b-data/preds_tform_mlp_1715280172.2843447.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('holdout predictions has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "preds_v2 = df['pred_Kd'].values\n",
    "\n",
    "\n",
    "# Histogram of predicted values\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.hist(preds, bins=100)\n",
    "plt.hist(preds, bins=100, alpha=1.0, label='Transformer v1', color='b')\n",
    "plt.hist(preds_v2, bins=100, alpha=1.0, label='Transformer v2', color='r')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.xlabel('pred Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of pred Kd values on the holdout set')\n",
    "plt.xlim((-1,5))\n",
    "# plt.ylim((0,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## alphaseq_data_train dataset  (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "data_file = './data/alphaseq_data_train.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('dataset has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "\n",
    "raw_Kds = df['Kd'].values\n",
    "\n",
    "# Histogram of Kd values\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(raw_Kds, bins=100)\n",
    "plt.xlabel('Experimental Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of Kd values in the raw training data set')\n",
    "plt.xlim((-1,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Kd distribution for clean-4 dataset train only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "# data_file = './data/q_cleaned_4_train_set.csv'\n",
    "data_file = './data/q_cleaned_4b_train_set.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('dataset has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "\n",
    "clean4_Kds = df['Kd'].values\n",
    "\n",
    "# Histogram of Kd values\n",
    "# plt.figure(figsize=(3,3))\n",
    "# plt.hist(clean4_Kds, bins=100)\n",
    "# plt.hist(raw_Kds, bins=100)\n",
    "plt.hist(raw_Kds, bins=100, alpha=1.0, label='raw data', color='b')\n",
    "plt.hist(clean4_Kds, bins=100, alpha=1.0, label='clean-4 data', color='r')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Experimental Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of experimental Kd values')\n",
    "plt.xlim((-1,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### MLP model  Clean-3b dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = ''\n",
    "plot_preds_hist(pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = './inference_results/mlp_model/cleaned-3/test_no_cls_token/preds_mlp_1714982629.5918856.pkl'\n",
    "true_file_path = './inference_results/mlp_model/cleaned-3/test_no_cls_token/y_mlp_1714982629.5921109.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "#\n",
    "pred_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/mlp_model/cleaned-3b-data/preds_mlp_1715105115.4793816.pkl'\n",
    "true_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/mlp_model/cleaned-3b-data/y_mlp_1715105115.4793816.pkl'\n",
    "\n",
    "pred = torch.tensor(pk.load(open(pred_file_path, 'rb'))).squeeze()\n",
    "true = torch.tensor(pk.load(open(true_file_path, 'rb'))).squeeze()\n",
    "\n",
    "print(pred.shape)\n",
    "print(true.shape)\n",
    "\n",
    "c = torch.stack((pred, true), dim=0)\n",
    "print(c.shape)\n",
    "\n",
    "p = torch.corrcoef(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Vision Transform Model (VIT)  1-channel Clean-3b Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = ''\n",
    "plot_preds_hist(pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on the validation set to compare actual with predicted values\n",
    "pred_file_path = './inference_results/vit_model/cleaned-3b/BW/test_no_cls_token/preds_vit_1715016846.793841.pkl'\n",
    "true_file_path = './inference_results/vit_model/cleaned-3b/BW/test_no_cls_token/y_vit_1715016846.7940617.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "#\n",
    "pred_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/vit_model/cleaned-3b-data/1-channel/preds_vit_1715105283.6443539.pkl'\n",
    "true_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/vit_model/cleaned-3b-data/1-channel/y_vit_1715105283.6443539.pkl'\n",
    "\n",
    "pred = torch.tensor(pk.load(open(pred_file_path, 'rb'))).squeeze()\n",
    "true = torch.tensor(pk.load(open(true_file_path, 'rb'))).squeeze()\n",
    "\n",
    "print(pred.shape)\n",
    "print(true.shape)\n",
    "\n",
    "c = torch.stack((pred, true), dim=0)\n",
    "print(c.shape)\n",
    "\n",
    "p = torch.corrcoef(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Vision Transform Model (VIT)  3-channel, Clean-3b Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on the validation set to compare actual with predicted values\n",
    "pred_file_path = './inference_results/vit_model/cleaned-3b/BGR/test_no_cls_token/preds_vit_1715020986.6452327.pkl'\n",
    "true_file_path = './inference_results/vit_model/cleaned-3b/BGR/test_no_cls_token/y_vit_1715020986.6455376.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "#\n",
    "pred_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/vit_model/cleaned-3b-data/3-channel/preds_vit_1715105421.8529866.pkl'\n",
    "true_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/vit_model/cleaned-3b-data/3-channel/y_vit_1715105421.8529866.pkl'\n",
    "\n",
    "pred = torch.tensor(pk.load(open(pred_file_path, 'rb'))).squeeze()\n",
    "true = torch.tensor(pk.load(open(true_file_path, 'rb'))).squeeze()\n",
    "\n",
    "print(pred.shape)\n",
    "print(true.shape)\n",
    "\n",
    "c = torch.stack((pred, true), dim=0)\n",
    "print(c.shape)\n",
    "\n",
    "p = torch.corrcoef(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TFormMLP Clean-4 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = './test_results/tform_mlp_model/cleaned-4-data/preds_tform_mlp_1715105469.0702462.pkl'\n",
    "true_file_path = './test_results/tform_mlp_model/cleaned-4-data/y_tform_mlp_1715105469.0702462.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "#\n",
    "pred_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/tform_mlp_model/cleaned-4-data/preds_tform_mlp_1715105469.0702462.pkl'\n",
    "true_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/tform_mlp_model/cleaned-4-data/y_tform_mlp_1715105469.0702462.pkl'\n",
    "\n",
    "pred = torch.tensor(pk.load(open(pred_file_path, 'rb'))).squeeze()\n",
    "true = torch.tensor(pk.load(open(true_file_path, 'rb'))).squeeze()\n",
    "\n",
    "print(pred.shape)\n",
    "print(true.shape)\n",
    "\n",
    "c = torch.stack((pred, true), dim=0)\n",
    "print(c.shape)\n",
    "\n",
    "p = torch.corrcoef(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TFormMLP Clean-4 Dataset:  pretrained, then fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = './test_results/tform_mlp_model/finetune/cleaned-4b-data/preds_tform_mlp_1715572603.424339.pkl'\n",
    "true_file_path = './test_results/tform_mlp_model/finetune/cleaned-4b-data/y_tform_mlp_1715572603.424339.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(-1.0, 0.5), ylim=(-0.2,0))\n",
    "\n",
    "a = pk.load(open(pred_file_path, 'rb'))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### t-SNE analysis\n",
    "\n",
    "The transformer-based models should have learned relationships between the elements of the sequence.  See how this appears in t-SNE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from torch.utils.data import DataLoader\n",
    "from models.tform_mlp import TFormMLP_Lightning\n",
    "from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "import os\n",
    "import yaml\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# Read the config\n",
    "config_path = './config/tform_mlp_params.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']    \n",
    "test_config = config['test_params']\n",
    "print(model_config)\n",
    "print(train_config)\n",
    "print(test_config)\n",
    "\n",
    "pl.seed_everything(config['seed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Load the dataset and dataloaders\n",
    "#----------------------------------------------------------\n",
    "test_data_path = './data/q_cleaned_4b_test_set.csv'\n",
    "test_dataset = dataset(train_config, model_config['block_size'], test_data_path, regularize=False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=train_config['batch_size'])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Load pre-trained model\n",
    "#----------------------------------------------------------\n",
    "checkpoint_name = './lightning_logs/tform_mlp_model/cleaned-4b-data/version_2/checkpoints/epoch=135-step=4200-val_loss=0.10-loss=0.09.ckpt' \n",
    "model = TFormMLP_Lightning.load_from_checkpoint(checkpoint_path=checkpoint_name, model_config=model_config, config=train_config)\n",
    "\n",
    "# register a forward hooks to pick out the output tensors for the attention blocks\n",
    "feats = {} #an empty dictionary\n",
    "def hook_func(m , inp ,op):\n",
    "   feats['feat'] = op.detach()\n",
    "\n",
    "model.model.transformer.encoders[5].attn_block.attend.register_forward_hook(hook_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test_loader)\n",
    "batch = next(it)\n",
    "# x, x2, x3, y, name = batch\n",
    "x, y, name = batch\n",
    "print('x shape:', x.shape, ', y shape:', y.shape)\n",
    "\n",
    "# y_hat, trans_out = model(x.to(model.device), x2.to(model.device), x3.to(model.device))\n",
    "y_hat, trans_out = model(x.to(model.device))\n",
    "print('y_hat shape:', y_hat.shape, ', trans_out shape', trans_out.shape)\n",
    "\n",
    "print('hook function results. feats[feat].shape:', feats['feat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "seq_idx = 100\n",
    "heads = feats['feat']\n",
    "head = heads[seq_idx,6,:,:].cpu()\n",
    "\n",
    "# calc the mean-head over all 8\n",
    "h1 = torch.sum(heads[10, :, :, :], dim=0)\n",
    "nhead = (torch.nn.functional.normalize(h1, p=2.0, dim=1)).cpu()\n",
    "print('nhead shape:', nhead.shape)\n",
    "\n",
    "h1_colsum = torch.sum(h1, dim=0)\n",
    "h1_colsum = torch.nn.functional.normalize(h1_colsum, p=2.0, dim=0)\n",
    "print(h1_colsum.shape)\n",
    "print(h1_colsum[28:112])\n",
    "x = np.arange(0, len(h1_colsum)) #28, 112, 1)\n",
    "y = h1_colsum.cpu().numpy()\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "cmap = plt.get_cmap('viridis')\n",
    "plt.imshow(nhead, cmap=cmap)  \n",
    "xmin=0 #28\n",
    "xmax=248 #112\n",
    "xticks = torch.arange(xmin, xmax, 2).tolist()\n",
    "plt.colorbar() \n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(xmax, xmin)\n",
    "# plt.xticks(xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "print('trans_out shape:', trans_out.shape)\n",
    "b, n, d = trans_out.shape\n",
    "print('b:', b, ', n:', n, ', d:', d)\n",
    "\n",
    "ctx_labels = torch.arange(0, model_config['block_size']+1)\n",
    "print('ctx_labels shape:', ctx_labels.shape)\n",
    "\n",
    "\n",
    "# Three types of tokens to distinguish in the tsne plot:\n",
    "# 1. classifier token (used in the regression)\n",
    "# 2. aa tokens in the CDR region\n",
    "# 3. all others (including the CLS and PAD tokens)\n",
    "ctx_cats = torch.ones_like(ctx_labels)  # label everything like it's a non-CDR aa group\n",
    "ctx_cats[0] = 0  # the classifier token used in regression\n",
    "ctx_cats[29:109] = 2  # the CDR region is aa residues 29-108\n",
    "\n",
    "print(ctx_cats)\n",
    "\n",
    "labels = ctx_cats.repeat(train_config['batch_size'])\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "ctx_vectors = torch.reshape(trans_out, (trans_out.shape[0]*trans_out.shape[1], trans_out.shape[2]))\n",
    "print('ctx_vectors shape', ctx_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just one sequence\n",
    "#\n",
    "num_seqs = 1\n",
    "labels = ctx_cats.repeat(num_seqs)\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "idx = 4\n",
    "one_seq = trans_out[idx:idx+num_seqs]\n",
    "print('shape one_seq:', one_seq.shape)\n",
    "new_shape = one_seq.shape[0]*one_seq.shape[1]\n",
    "\n",
    "ctx_vectors = one_seq\n",
    "ctx_vectors = torch.reshape(one_seq, (one_seq.shape[0]*one_seq.shape[1], one_seq.shape[2]))\n",
    "print('ctx_vectors shape', ctx_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "n_iter = 10000\n",
    "\n",
    "# 2D\n",
    "tsne = TSNE(n_components=2, random_state=config['seed'], metric=\"cosine\", n_iter=n_iter, verbose=True)\n",
    "x_tsne = tsne.fit_transform(ctx_vectors.detach().cpu().numpy())\n",
    "print('x_tsne shape:', x_tsne.shape)\n",
    "print(tsne.kl_divergence_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path('./misc_analysis/tSNE/tform_mlp/cleaned-4b-data')\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# pk.dump(x_tsne, open('./misc_analysis/tSNE/tform_mlp/cleaned-4b-data/tsne_x_10000iter_tform_mlp_v2.pkl', 'wb'))\n",
    "\n",
    "# x_tsne = pk.load(open('./misc_analysis/tSNE/tform_mlp_v2/tsne_x_10000iter_tform_mlp_v2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an array with three colors\n",
    "# colors = [\"#FF0000\", \"#00FF00\", \"#0000FF\", \"#000000\"] # rgb, black\n",
    "colors = [\"#0000FF\", \"#A0A0A0\", \"#FF0000\"] # rgb\n",
    "# Set custom color palette\n",
    "my_palette = sns.color_palette(colors)\n",
    "sns.set_palette(my_palette)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "x_all = x_tsne[:, 0] \n",
    "y_all = x_tsne[:, 1] \n",
    "cat_all = labels     \n",
    "plt.title('t-SNE of transformer model output')  \n",
    "ax.set_xlabel(\"t-SNE y\")\n",
    "ax.set_ylabel(\"t-SNE x\")\n",
    "p = sns.scatterplot(x=x_all, y=y_all, alpha=0.6, ax=ax, palette=sns.color_palette(my_palette), hue=labels, legend=True)\n",
    "\n",
    "# title\n",
    "p.legend_.set_title('Token type')\n",
    "new_labels = ['regression token', 'aa: non-CDR', 'aa: CDR region (29-108)']\n",
    "for t, l in zip(p.legend_.texts, new_labels):\n",
    "    t.set_text(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the color palette\n",
    "sns.set_palette(sns.color_palette(\"Paired\"))\n",
    "# Plot the data, specifying a different color for data points in\n",
    "# each of the day categories (weekday and weekend)\n",
    "ax = sns.scatterplot(x='day', y='miles_walked', data=dataset, hue='day_category')\n",
    "# Customize the axes and title\n",
    "ax.set_title(\"Miles walked\")\n",
    "ax.set_xlabel(\"day\")\n",
    "ax.set_ylabel(\"total miles\")\n",
    "# Remove top and right borders\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Analysis of output of attention blocks to see the interactions\n",
    "\n",
    "### (also, see the BertViz tool:  https://github.com/jessevig/bertviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from torch.utils.data import DataLoader\n",
    "from models.tform_mlp_v2 import TFormMLP_Lightning_v2\n",
    "from datasets.scFv_dataset_v2 import scFv_Dataset_v2 as dataset\n",
    "import os\n",
    "import yaml\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# Read the config\n",
    "config_path = './config/tform_mlp_params.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']    \n",
    "test_config = config['test_params']\n",
    "print(model_config)\n",
    "print(train_config)\n",
    "print(test_config)\n",
    "\n",
    "pl.seed_everything(config['seed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Load the dataset and dataloaders\n",
    "#----------------------------------------------------------\n",
    "test_data_path = './data/q_cleaned_4b_test_set.csv'\n",
    "test_dataset = dataset(train_config, model_config['block_size'], test_data_path, regularize=False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=train_config['batch_size'])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Load pre-trained model\n",
    "#----------------------------------------------------------\n",
    "checkpoint_name = './lightning_logs/tform_mlp_model/cleaned-4-data/trained/checkpoints/epoch=316-step=9800-val_loss=0.11-loss=0.05.ckpt' \n",
    "model = TFormMLP_Lightning_v2.load_from_checkpoint(checkpoint_path=checkpoint_name, model_config=model_config, config=train_config)\n",
    "\n",
    "\n",
    "it = iter(test_loader)\n",
    "batch = next(it)\n",
    "x, y, name = batch\n",
    "print('x shape:', x.shape, ', y shape:', y.shape)\n",
    "\n",
    "y_hat, trans_out = model(x.to(model.device))\n",
    "print('y_hat shape:', y_hat.shape, ', trans_out shape', trans_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Use forward hooks to pick out the output tensors for the attention blocks\n",
    "#\n",
    "\n",
    "feats = {} #an empty dictionary\n",
    "def hook_func(m , inp ,op):\n",
    "   feats['feat'] = op.detach()\n",
    "\n",
    "# it = iter(test_loader)\n",
    "# batch = next(it)\n",
    "# x, x2, x3, y, name = batch\n",
    "y_hat, trans_out = model(x.to(model.device), x2.to(model.device), x3.to(model.device))\n",
    "\n",
    "print(feats['feat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks to pick outputs from specific layers in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
