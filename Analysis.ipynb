{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### Mark Thompson\n",
    "### Started April 29, 2024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Some plotting functions\n",
    "#\n",
    "def plot_preds_hist(preds_file_path):\n",
    "    preds = pk.load(open(preds_file_path, 'rb'))\n",
    "    print('len(preds):', len(preds))\n",
    "    preds = [p[0] for p in preds]\n",
    "    print('preds[0:10]:', preds[0:10])\n",
    "\n",
    "    # Histogram of predicted values\n",
    "    plt.hist(preds, bins=100)\n",
    "    plt.xlabel('pred Kd (nm)')\n",
    "    plt.ylabel('count')\n",
    "    plt.title('Distribution of pred values set')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred_vs_true(preds_file_path, true_file_path, xlim=(0,5), ylim=(0,5)):\n",
    "    preds = pk.load(open(preds_file_path, 'rb'))\n",
    "    y = pk.load(open(true_file_path, 'rb'))\n",
    "    print('len(preds):', len(preds), ', len(y):', len(y))\n",
    "    preds = [p[0] for p in preds]\n",
    "    y = [a[0] for a in y]\n",
    "\n",
    "    # scatter plot of true vs pred\n",
    "    plt.scatter(y, preds, c =\"blue\")\n",
    "    plt.xlabel('experimental Kd (nm)')\n",
    "    plt.ylabel('predicted Kd (nm)')\n",
    "    plt.title('true vs predicted Kd on validation set')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Holdout dataset and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The holdout data\n",
    "data_file = './data/alphaseq_data_hold_out.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('holdout dataframe has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df['sequence_a'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "data_file = './inference_results/tform_mlp_model/cleaned-4-data/preds_vit_1715104590.5575511.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('holdout predictions has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "\n",
    "preds = df['pred_Kd'].values\n",
    "\n",
    "# Histogram of predicted values\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(preds, bins=100)\n",
    "plt.xlabel('pred Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of pred Kd values on the holdout set')\n",
    "plt.xlim((-1,5))\n",
    "# plt.ylim((0,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## alphaseq_data_train dataset  (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "data_file = './data/alphaseq_data_train.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('dataset has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "\n",
    "raw_Kds = df['Kd'].values\n",
    "\n",
    "# Histogram of Kd values\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(raw_Kds, bins=100)\n",
    "plt.xlabel('Experimental Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of Kd values in the raw training data set')\n",
    "plt.xlim((-1,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Kd distribution for clean-4 dataset train only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "data_file = './data/q_cleaned_4_train_set.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('dataset has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "\n",
    "clean4_Kds = df['Kd'].values\n",
    "\n",
    "# Histogram of Kd values\n",
    "# plt.figure(figsize=(3,3))\n",
    "# plt.hist(clean4_Kds, bins=100)\n",
    "# plt.hist(raw_Kds, bins=100)\n",
    "plt.hist(raw_Kds, bins=100, alpha=1.0, label='raw data', color='b')\n",
    "plt.hist(clean4_Kds, bins=100, alpha=1.0, label='clean-4 data', color='r')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Experimental Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of experimental Kd values')\n",
    "plt.xlim((-1,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### MLP model  Clean-3b dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = ''\n",
    "plot_preds_hist(pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = './inference_results/mlp_model/cleaned-3/test_no_cls_token/preds_mlp_1714982629.5918856.pkl'\n",
    "true_file_path = './inference_results/mlp_model/cleaned-3/test_no_cls_token/y_mlp_1714982629.5921109.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Vision Transform Model (VIT)  1-channel Clean-3b Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = ''\n",
    "plot_preds_hist(pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on the validation set to compare actual with predicted values\n",
    "pred_file_path = './inference_results/vit_model/cleaned-3b/BW/test_no_cls_token/preds_vit_1715016846.793841.pkl'\n",
    "true_file_path = './inference_results/vit_model/cleaned-3b/BW/test_no_cls_token/y_vit_1715016846.7940617.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Vision Transform Model (VIT)  3-channel, Clean-3b Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on the validation set to compare actual with predicted values\n",
    "pred_file_path = './inference_results/vit_model/cleaned-3b/BGR/test_no_cls_token/preds_vit_1715020986.6452327.pkl'\n",
    "true_file_path = './inference_results/vit_model/cleaned-3b/BGR/test_no_cls_token/y_vit_1715020986.6455376.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TFormMLP Clean-4 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = './test_results/tform_mlp_model/cleaned-4-data/preds_tform_mlp_1715105469.0702462.pkl'\n",
    "true_file_path = './test_results/tform_mlp_model/cleaned-4-data/y_tform_mlp_1715105469.0702462.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### t-SNE analysis\n",
    "\n",
    "The transformer-based models should have learned relationships between the elements of the sequence.  See how this appears in t-SNE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from torch.utils.data import DataLoader\n",
    "from models.tform_mlp.tform_mlp import TFormMLP_Lightning\n",
    "from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "import os\n",
    "import yaml\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# Read the config\n",
    "config_path = './config/tform_mlp_params.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']    \n",
    "test_config = config['test_params']\n",
    "print(model_config)\n",
    "print(train_config)\n",
    "print(test_config)\n",
    "\n",
    "pl.seed_everything(config['seed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Load the dataset and dataloaders\n",
    "#----------------------------------------------------------\n",
    "test_data_path = './data/q_cleaned_4_test_set.csv'\n",
    "test_dataset = dataset(train_config, model_config['block_size'], test_data_path, regularize=False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=train_config['batch_size'])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Load pre-trained model\n",
    "#----------------------------------------------------------\n",
    "checkpoint_name = './lightning_logs/tform_mlp_model/cleaned-4-data/trained/checkpoints/epoch=316-step=9800-val_loss=0.11-loss=0.05.ckpt' \n",
    "model = TFormMLP_Lightning.load_from_checkpoint(checkpoint_path=checkpoint_name, model_config=model_config, config=train_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test_loader)\n",
    "batch = next(it)\n",
    "x, y = batch\n",
    "print('x shape:', x.shape, ', y shape:', y.shape)\n",
    "\n",
    "y_hat, trans_out = model(x.to(model.device))\n",
    "print('y_hat shape:', y_hat.shape, ', trans_out shape', trans_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "print('trans_out shape:', trans_out.shape)\n",
    "b, n, d = trans_out.shape\n",
    "print('b:', b, ', n:', n, ', d:', d)\n",
    "\n",
    "ctx_labels = torch.arange(0, model_config['block_size']+1)\n",
    "print('ctx_labels shape:', ctx_labels.shape)\n",
    "\n",
    "\n",
    "# Three types of tokens to distinguish in the tsne plot:\n",
    "# 1. classifier token (used in the regression)\n",
    "# 2. aa tokens in the CDR region\n",
    "# 3. all others (including the CLS and PAD tokens)\n",
    "ctx_cats = torch.ones_like(ctx_labels)  # label everything like it's a non-CDR aa group\n",
    "ctx_cats[0] = 0  # the classifier token used in regression\n",
    "ctx_cats[29:109] = 2  # the CDR region is aa residues 29-108\n",
    "\n",
    "print(ctx_cats)\n",
    "\n",
    "labels = ctx_cats.repeat(train_config['batch_size'])\n",
    "print('labels shape:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_vectors = torch.reshape(trans_out, (trans_out.shape[0]*trans_out.shape[1], trans_out.shape[2]))\n",
    "print('ctx_vectors shape', ctx_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "n_iter = 10000\n",
    "\n",
    "# 2D\n",
    "tsne = TSNE(n_components=2, random_state=config['seed'], metric=\"cosine\", n_iter=n_iter, verbose=True)\n",
    "x_tsne = tsne.fit_transform(ctx_vectors.detach().cpu().numpy())\n",
    "print('x_tsne shape:', x_tsne.shape)\n",
    "print(tsne.kl_divergence_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path('./misc_analysis/tSNE/tform_mlp/')\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pk.dump(x_tsne, open('./misc_analysis/tSNE/tform_mlp/tsne_x_10000iter_tform_mlp.pkl', 'wb'))\n",
    "\n",
    "# x_tsne = pk.load(open('./misc_analysis/tform_mlp/tsne_x_10000iter_tform_mlp.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an array with three colors\n",
    "# colors = [\"#FF0000\", \"#00FF00\", \"#0000FF\", \"#000000\"] # rgb, black\n",
    "colors = [\"#0000FF\", \"#A0A0A0\", \"#FF0000\"] # rgb\n",
    "# Set custom color palette\n",
    "my_palette = sns.color_palette(colors)\n",
    "sns.set_palette(my_palette)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "x_all = x_tsne[:, 0] \n",
    "y_all = x_tsne[:, 1] \n",
    "cat_all = labels     \n",
    "plt.title('t-SNE of transformer model output tensors')  \n",
    "ax.set_xlabel(\"t-SNE y\")\n",
    "ax.set_ylabel(\"t-SNE x\")\n",
    "p = sns.scatterplot(x=x_all, y=y_all, alpha=0.6, ax=ax, palette=sns.color_palette(my_palette), hue=labels, legend=True)\n",
    "\n",
    "# title\n",
    "p.legend_.set_title('Token type')\n",
    "new_labels = ['regression token', 'aa: non-CDR', 'aa: CDR region (29-108)']\n",
    "for t, l in zip(p.legend_.texts, new_labels):\n",
    "    t.set_text(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the color palette\n",
    "sns.set_palette(sns.color_palette(\"Paired\"))\n",
    "# Plot the data, specifying a different color for data points in\n",
    "# each of the day categories (weekday and weekend)\n",
    "ax = sns.scatterplot(x='day', y='miles_walked', data=dataset, hue='day_category')\n",
    "# Customize the axes and title\n",
    "ax.set_title(\"Miles walked\")\n",
    "ax.set_xlabel(\"day\")\n",
    "ax.set_ylabel(\"total miles\")\n",
    "# Remove top and right borders\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Analysis of output of attention blocks to see the interactions\n",
    "\n",
    "### (also, see the BertViz tool:  https://github.com/jessevig/bertviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from torch.utils.data import DataLoader\n",
    "from models.tform_mlp import TFormMLP_Lightning\n",
    "from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "import os\n",
    "import yaml\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# Read the config\n",
    "config_path = './config/tform_mlp_params.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']    \n",
    "test_config = config['test_params']\n",
    "print(model_config)\n",
    "print(train_config)\n",
    "print(test_config)\n",
    "\n",
    "pl.seed_everything(config['seed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Load the dataset and dataloaders\n",
    "#----------------------------------------------------------\n",
    "test_data_path = './data/q_cleaned_4_test_set.csv'\n",
    "test_dataset = dataset(train_config, model_config['block_size'], test_data_path, regularize=False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=train_config['batch_size'])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Load pre-trained model\n",
    "#----------------------------------------------------------\n",
    "checkpoint_name = './lightning_logs/tform_mlp_model/cleaned-4-data/trained/checkpoints/epoch=316-step=9800-val_loss=0.11-loss=0.05.ckpt' \n",
    "model = TFormMLP_Lightning.load_from_checkpoint(checkpoint_path=checkpoint_name, model_config=model_config, config=train_config)\n",
    "\n",
    "\n",
    "it = iter(test_loader)\n",
    "batch = next(it)\n",
    "x, y, name = batch\n",
    "print('x shape:', x.shape, ', y shape:', y.shape)\n",
    "\n",
    "y_hat, trans_out = model(x.to(model.device))\n",
    "print('y_hat shape:', y_hat.shape, ', trans_out shape', trans_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Use forward hooks to pick out the output tensors for the attention blocks\n",
    "#\n",
    "\n",
    "feats = {} #an empty dictionary\n",
    "def hook_func(m , inp ,op):\n",
    "   feats['feat'] = op.detach()\n",
    "\n",
    "model.model.transformer.encoders[5].attn_block.attend.register_forward_hook(hook_func)\n",
    "y_hat, trans_out = model(x.to(model.device))\n",
    "print(feats['feat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks to pick outputs from specific layers in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
