{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### Mark Thompson\n",
    "### Started April 29, 2024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Some plotting functions\n",
    "#\n",
    "def plot_preds_hist(preds_file_path):\n",
    "    preds = pk.load(open(preds_file_path, 'rb'))\n",
    "    print('len(preds):', len(preds))\n",
    "    preds = [p[0] for p in preds]\n",
    "    print('preds[0:10]:', preds[0:10])\n",
    "\n",
    "    # Histogram of predicted values\n",
    "    plt.hist(preds, bins=100)\n",
    "    plt.xlabel('pred Kd (nm)')\n",
    "    plt.ylabel('count')\n",
    "    plt.title('Distribution of pred values set')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred_vs_true(preds_file_path, true_file_path, xlim=(0,5), ylim=(0,5)):\n",
    "    preds = pk.load(open(preds_file_path, 'rb'))\n",
    "    y = pk.load(open(true_file_path, 'rb'))\n",
    "    print('len(preds):', len(preds), ', len(y):', len(y))\n",
    "    preds = [p[0] for p in preds]\n",
    "    y = [a[0] for a in y]\n",
    "\n",
    "    # scatter plot of true vs pred\n",
    "    plt.scatter(y, preds, c =\"blue\")\n",
    "    plt.xlabel('experimental Kd (nm)')\n",
    "    plt.ylabel('predicted Kd (nm)')\n",
    "    plt.title('true vs predicted Kd on validation set')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Holdout dataset and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The holdout data\n",
    "data_file = './data/alphaseq_data_hold_out.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('holdout dataframe has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df['sequence_a'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "\n",
    "# tform_mlp version 1 predictions\n",
    "data_file = './inference_results/tform_mlp_model/cleaned-4-data/preds_tform_mlp_1715104590.5575511.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('holdout predictions has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "preds = df['pred_Kd'].values\n",
    "\n",
    "# tform_mlp_v2 version 2 predictions\n",
    "data_file = './inference_results/tform_mlp_model_v2/addendum/cleaned-4b-data/preds_tform_mlp_1715280172.2843447.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('holdout predictions has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "preds_v2 = df['pred_Kd'].values\n",
    "\n",
    "\n",
    "# Histogram of predicted values\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.hist(preds, bins=100)\n",
    "plt.hist(preds, bins=100, alpha=1.0, label='Transformer v1', color='b')\n",
    "plt.hist(preds_v2, bins=100, alpha=1.0, label='Transformer v2', color='r')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.xlabel('pred Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of pred Kd values on the holdout set')\n",
    "plt.xlim((-1,5))\n",
    "# plt.ylim((0,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## alphaseq_data_train dataset  (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "data_file = './data/alphaseq_data_train.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('dataset has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "\n",
    "raw_Kds = df['Kd'].values\n",
    "\n",
    "# Histogram of Kd values\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(raw_Kds, bins=100)\n",
    "plt.xlabel('Experimental Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of Kd values in the raw training data set')\n",
    "plt.xlim((-1,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Kd distribution for clean-4 dataset train only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions on the holdout set\n",
    "# data_file = './data/q_cleaned_4_train_set.csv'\n",
    "data_file = './data/q_cleaned_4b_train_set.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "rows1 = df.shape[0]\n",
    "print('dataset has', rows1, 'rows')\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "\n",
    "clean4_Kds = df['Kd'].values\n",
    "\n",
    "# Histogram of Kd values\n",
    "# plt.figure(figsize=(3,3))\n",
    "# plt.hist(clean4_Kds, bins=100)\n",
    "# plt.hist(raw_Kds, bins=100)\n",
    "plt.hist(raw_Kds, bins=100, alpha=1.0, label='raw data', color='b')\n",
    "plt.hist(clean4_Kds, bins=100, alpha=1.0, label='clean-4 data', color='r')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Experimental Kd (nm)')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribution of experimental Kd values')\n",
    "plt.xlim((-1,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### MLP model  Clean-3b dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = ''\n",
    "plot_preds_hist(pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = './inference_results/mlp_model/cleaned-3/test_no_cls_token/preds_mlp_1714982629.5918856.pkl'\n",
    "true_file_path = './inference_results/mlp_model/cleaned-3/test_no_cls_token/y_mlp_1714982629.5921109.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "#\n",
    "pred_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/mlp_model/cleaned-3b-data/preds_mlp_1715105115.4793816.pkl'\n",
    "true_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/mlp_model/cleaned-3b-data/y_mlp_1715105115.4793816.pkl'\n",
    "\n",
    "pred = torch.tensor(pk.load(open(pred_file_path, 'rb'))).squeeze()\n",
    "true = torch.tensor(pk.load(open(true_file_path, 'rb'))).squeeze()\n",
    "\n",
    "print(pred.shape)\n",
    "print(true.shape)\n",
    "\n",
    "c = torch.stack((pred, true), dim=0)\n",
    "print(c.shape)\n",
    "\n",
    "p = torch.corrcoef(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "### Vision Transform Model (VIT)  1-channel Clean-3b Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = ''\n",
    "plot_preds_hist(pred_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on the validation set to compare actual with predicted values\n",
    "pred_file_path = './inference_results/vit_model/cleaned-3b/BW/test_no_cls_token/preds_vit_1715016846.793841.pkl'\n",
    "true_file_path = './inference_results/vit_model/cleaned-3b/BW/test_no_cls_token/y_vit_1715016846.7940617.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "#\n",
    "pred_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/vit_model/cleaned-3b-data/1-channel/preds_vit_1715105283.6443539.pkl'\n",
    "true_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/vit_model/cleaned-3b-data/1-channel/y_vit_1715105283.6443539.pkl'\n",
    "\n",
    "pred = torch.tensor(pk.load(open(pred_file_path, 'rb'))).squeeze()\n",
    "true = torch.tensor(pk.load(open(true_file_path, 'rb'))).squeeze()\n",
    "\n",
    "print(pred.shape)\n",
    "print(true.shape)\n",
    "\n",
    "c = torch.stack((pred, true), dim=0)\n",
    "print(c.shape)\n",
    "\n",
    "p = torch.corrcoef(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Vision Transform Model (VIT)  3-channel, Clean-3b Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on the validation set to compare actual with predicted values\n",
    "pred_file_path = './inference_results/vit_model/cleaned-3b/BGR/test_no_cls_token/preds_vit_1715020986.6452327.pkl'\n",
    "true_file_path = './inference_results/vit_model/cleaned-3b/BGR/test_no_cls_token/y_vit_1715020986.6455376.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "#\n",
    "pred_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/vit_model/cleaned-3b-data/3-channel/preds_vit_1715105421.8529866.pkl'\n",
    "true_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/vit_model/cleaned-3b-data/3-channel/y_vit_1715105421.8529866.pkl'\n",
    "\n",
    "pred = torch.tensor(pk.load(open(pred_file_path, 'rb'))).squeeze()\n",
    "true = torch.tensor(pk.load(open(true_file_path, 'rb'))).squeeze()\n",
    "\n",
    "print(pred.shape)\n",
    "print(true.shape)\n",
    "\n",
    "c = torch.stack((pred, true), dim=0)\n",
    "print(c.shape)\n",
    "\n",
    "p = torch.corrcoef(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TFormMLP Clean-4 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = './test_results/tform_mlp_model/cleaned-4-data/preds_tform_mlp_1715105469.0702462.pkl'\n",
    "true_file_path = './test_results/tform_mlp_model/cleaned-4-data/y_tform_mlp_1715105469.0702462.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(0,3.5), ylim=(0,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation Coefficient\n",
    "#\n",
    "pred_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/tform_mlp_model/cleaned-4-data/preds_tform_mlp_1715105469.0702462.pkl'\n",
    "true_file_path = '/Users/markthompson/Documents/dev/a-alphaBio-homework/test_results/tform_mlp_model/cleaned-4-data/y_tform_mlp_1715105469.0702462.pkl'\n",
    "\n",
    "pred = torch.tensor(pk.load(open(pred_file_path, 'rb'))).squeeze()\n",
    "true = torch.tensor(pk.load(open(true_file_path, 'rb'))).squeeze()\n",
    "\n",
    "print(pred.shape)\n",
    "print(true.shape)\n",
    "\n",
    "c = torch.stack((pred, true), dim=0)\n",
    "print(c.shape)\n",
    "\n",
    "p = torch.corrcoef(c)\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TFormMLP Clean-4 Dataset:  pretrained, then fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file_path = './test_results/tform_mlp_model/finetune/cleaned-4b-data/preds_tform_mlp_1715572603.424339.pkl'\n",
    "true_file_path = './test_results/tform_mlp_model/finetune/cleaned-4b-data/y_tform_mlp_1715572603.424339.pkl'\n",
    "plot_pred_vs_true(pred_file_path, true_file_path, xlim=(-1.0, 0.5), ylim=(-0.2,0))\n",
    "\n",
    "a = pk.load(open(pred_file_path, 'rb'))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### t-SNE analysis\n",
    "\n",
    "The transformer-based models should have learned relationships between the elements of the sequence.  See how this appears in t-SNE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from torch.utils.data import DataLoader\n",
    "from models.tform import TFormMLP_Lightning\n",
    "from datasets.scFv_diy_dataset_pretrain import scFv_diy_pretrain_Dataset as dataset\n",
    "import os\n",
    "import yaml\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# Read the config\n",
    "config_path = './config/tform_params.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']    \n",
    "test_config = config['test_params']\n",
    "print(model_config)\n",
    "print(train_config)\n",
    "print(test_config)\n",
    "\n",
    "pl.seed_everything(config['seed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Load the dataset and dataloaders\n",
    "#----------------------------------------------------------\n",
    "test_data_path = './data/oas_2/paired/paired_2_val_set.csv'\n",
    "test_dataset = dataset(train_config, model_config['block_size'], test_data_path, regularize=False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=256) #train_config['batch_size'])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Load pre-trained model\n",
    "#----------------------------------------------------------\n",
    "checkpoint_name = './lightning_logs/tform_mlp_model/pretrain/big2/chain_id/paired_2_set/version_1/checkpoints/epoch=60-step=8000-val_loss=0.44-loss=0.43.ckpt' \n",
    "model = TFormMLP_Lightning.load_from_checkpoint(checkpoint_path=checkpoint_name, model_config=model_config, config=train_config)\n",
    "\n",
    "# register a forward hooks to pick out the output tensors for the attention blocks\n",
    "feats = {} #an empty dictionary\n",
    "def hook_func(m , inp ,op):\n",
    "   feats['feat'] = op.detach()\n",
    "\n",
    "# The hook will grab the output of the final Encoder layer's attention block\n",
    "model.model.transformer.encoders[-1].attn_block.attend.register_forward_hook(hook_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test_loader)\n",
    "batch = next(it)\n",
    "# x, x2, x3, y, name = batch\n",
    "x, chain, mask, y, name = batch\n",
    "print('x shape:', x.shape, ', chain shape:', chain.shape, ', mask shape:', mask.shape, ', y shape:', y.shape)\n",
    "\n",
    "y_hat, loss, trans_out = model(x.to(model.device), chain.to(model.device), mask.to(model.device))\n",
    "print('y_hat shape:', y_hat.shape, ', trans_out shape', trans_out.shape)\n",
    "\n",
    "print('hook function results. feats[feat].shape:', feats['feat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "seq_idx = 1\n",
    "# print('chain:', chain[seq_idx])\n",
    "heads = feats['feat']\n",
    "print('heads shape:', heads.shape)\n",
    "nhead = heads[seq_idx,2,:,:].cpu()\n",
    "print('nhead shape:', nhead.shape)\n",
    "\n",
    "sum = torch.sum(nhead[10,:])\n",
    "print(sum)\n",
    "\n",
    "\n",
    "# calc the mean-head over all heads\n",
    "# h1 = torch.sum(heads[seq_idx, :, :, :], dim=0)\n",
    "# nhead = (torch.nn.functional.normalize(h1, p=2.0, dim=1)).cpu()\n",
    "print('nhead shape:', nhead.shape)\n",
    "\n",
    "# h1_colsum = torch.sum(h1, dim=0)\n",
    "# h1_colsum = torch.nn.functional.normalize(h1_colsum, p=2.0, dim=0)\n",
    "# print(h1_colsum)\n",
    "# print(h1_colsum.shape)\n",
    "# print(h1_colsum[28:112])\n",
    "# x = np.arange(0, len(h1_colsum)) #28, 112, 1)\n",
    "# y = h1_colsum.cpu().numpy()\n",
    "# plt.plot(x,y)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "cmap = plt.get_cmap('brg_r') #viridis')\n",
    "plt.imshow(nhead, cmap=cmap)  \n",
    "xmin=48 #0 \n",
    "xmax=50 #248 \n",
    "xticks = torch.arange(xmin, xmax, 2).tolist()\n",
    "# plt.colorbar() \n",
    "# plt.title('Attention map for head #2 in the final encoder layer\\nfor the first sequence in the batch')  \n",
    "# plt.title('Attention map for head #2 in the final encoder layer\\nThe linker region\\nfor the first sequence in the batch')  \n",
    "# plt.title('Attention map for head #2 in the final encoder layer\\nHeavy Chain region\\nfor the first sequence in the batch')  \n",
    "plt.xlim(0,  60) #248)  #xmin, xmax)\n",
    "plt.ylim(xmax, xmin)\n",
    "plt.xlabel('aa residue')\n",
    "plt.ylabel('aa residue')\n",
    "plt.xticks(torch.arange(1, 60, 2).tolist())\n",
    "plt.yticks([48, 49, 50])\n",
    "\n",
    "for i in range(0, 60):\n",
    "    print('{:.3f}'.format(nhead[49,i]), end=' ')\n",
    "\n",
    "# print('{:.2f}'.format([nhead[49, 0:60]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "seq_idx = 1\n",
    "\n",
    "# 'rgb' is not a valid value for name; supported values are 'Accent',\n",
    "# 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', \n",
    "# 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', \n",
    "# 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', \n",
    "# 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', \n",
    "# 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', \n",
    "# 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', \n",
    "# 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', \n",
    "# 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', \n",
    "# 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', \n",
    "# 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', \n",
    "# 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', \n",
    "# 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', \n",
    "# 'crest', 'crest_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'flare', 'flare_r', \n",
    "# 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', \n",
    "# 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', \n",
    "# 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', \n",
    "# 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'jet', 'jet_r', \n",
    "# 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', \n",
    "# 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket',\n",
    "#  'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', \n",
    "# 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', \n",
    "# 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', \n",
    "# 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r\n",
    "\n",
    "cols = 4\n",
    "rows = 4\n",
    "num_heads = 16\n",
    "print('num rows:', rows, ', num cols:', cols)\n",
    "plt.figure(figsize=(15,15))\n",
    "cmap = plt.get_cmap('brg') #viridis') brg_r\n",
    "\n",
    "idx = 1\n",
    "for hidx in range(num_heads):\n",
    "    nhead = heads[seq_idx,hidx,:,:].cpu()  \n",
    "    ax = plt.subplot(rows, cols, idx)\n",
    "    ax.axis('off')\n",
    "    plt.imshow(nhead, cmap=cmap)  \n",
    "    idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "print('trans_out shape:', trans_out.shape)\n",
    "b, n, d = trans_out.shape\n",
    "print('b:', b, ', n:', n, ', d:', d)\n",
    "\n",
    "ctx_labels = torch.arange(0, model_config['block_size'])\n",
    "print('ctx_labels shape:', ctx_labels.shape)\n",
    "\n",
    "\n",
    "# Five types of tokens to distinguish in the tsne plot:\n",
    "# chain id to distinguish heavy, light, and linker\n",
    "# 1: heavy chain, 2: light chain, 3: linker.  CLS: 0, PAD: 4\n",
    "# ctx_cats = torch.ones_like(ctx_labels)  # label everything like it's a non-CDR aa group\n",
    "# ctx_cats[0] = 0  # the classifier token used in regression\n",
    "# ctx_cats[29:109] = 2  # the CDR region is aa residues 29-108\n",
    "labels = chain.squeeze()\n",
    "print('labels shape:', labels.shape)\n",
    "# print(labels)\n",
    "\n",
    "# labels = ctx_cats.repeat(train_config['batch_size'])\n",
    "# print('labels shape:', labels.shape)\n",
    "\n",
    "ctx_vectors = torch.reshape(trans_out, (trans_out.shape[0]*trans_out.shape[1], trans_out.shape[2]))\n",
    "labels = torch.flatten(labels)\n",
    "print('ctx_vectors shape', ctx_vectors.shape)\n",
    "print('labels shape', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just one sequence\n",
    "#\n",
    "num_seqs = 1\n",
    "labels = ctx_cats.repeat(num_seqs)\n",
    "print('labels shape:', labels.shape)\n",
    "\n",
    "idx = 4\n",
    "one_seq = trans_out[idx:idx+num_seqs]\n",
    "print('shape one_seq:', one_seq.shape)\n",
    "new_shape = one_seq.shape[0]*one_seq.shape[1]\n",
    "\n",
    "ctx_vectors = one_seq\n",
    "ctx_vectors = torch.reshape(one_seq, (one_seq.shape[0]*one_seq.shape[1], one_seq.shape[2]))\n",
    "print('ctx_vectors shape', ctx_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "n_iter = 3000\n",
    "\n",
    "# 2D\n",
    "tsne = TSNE(n_components=2, random_state=config['seed'], metric=\"cosine\", n_iter=n_iter, verbose=True)\n",
    "x_tsne = tsne.fit_transform(ctx_vectors.detach().cpu().numpy())\n",
    "print('x_tsne shape:', x_tsne.shape)\n",
    "print(tsne.kl_divergence_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path('./misc_analysis/tSNE/tform_mlp/pretrain/big2/chain_id/paired_2_set/')\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pk.dump(x_tsne, open(os.path.join(path, 'tsne_10000iter_tform_pretrain_paired2.pkl'), 'wb'))\n",
    "\n",
    "# x_tsne = pk.load(open('./misc_analysis/tSNE/tform_mlp_v2/tsne_x_10000iter_tform_mlp_v2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an array with three colors\n",
    "colors = [\"#55FF00\", \"#FF0000\", \"#0000FF\", \"#aa00FF\",  \"#000000\"] # rgb, purple, and black\n",
    "# colors = [\"#0000FF\", \"#A0A0A0\", \"#FF0000\"] # rgb\n",
    "# Set custom color palette\n",
    "my_palette = sns.color_palette(colors)\n",
    "sns.set_palette(my_palette)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "x_all = x_tsne[:, 0] \n",
    "y_all = x_tsne[:, 1] \n",
    "cat_all = labels     \n",
    "plt.title('t-SNE of transformer model output')  \n",
    "ax.set_xlabel(\"t-SNE y\")\n",
    "ax.set_ylabel(\"t-SNE x\")\n",
    "p = sns.scatterplot(x=x_all, y=y_all, alpha=0.6, ax=ax, palette=sns.color_palette(my_palette), hue=labels, legend=True)\n",
    "\n",
    "# title\n",
    "p.legend_.set_title('Token type')\n",
    "new_labels = ['regression token', 'aa: non-CDR', 'aa: CDR region (29-108)']\n",
    "new_labels = ['regression token', 'heavy chain', 'light chain', 'linker',  'PAD token']\n",
    "for t, l in zip(p.legend_.texts, new_labels):\n",
    "    t.set_text(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the color palette\n",
    "sns.set_palette(sns.color_palette(\"Paired\"))\n",
    "# Plot the data, specifying a different color for data points in\n",
    "# each of the day categories (weekday and weekend)\n",
    "ax = sns.scatterplot(x='day', y='miles_walked', data=dataset, hue='day_category')\n",
    "# Customize the axes and title\n",
    "ax.set_title(\"Miles walked\")\n",
    "ax.set_xlabel(\"day\")\n",
    "ax.set_ylabel(\"total miles\")\n",
    "# Remove top and right borders\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Analysis of output of attention blocks to see the interactions\n",
    "\n",
    "### (also, see the BertViz tool:  https://github.com/jessevig/bertviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from torch.utils.data import DataLoader\n",
    "from models.tform_mlp_v2 import TFormMLP_Lightning_v2\n",
    "from datasets.scFv_dataset_v2 import scFv_Dataset_v2 as dataset\n",
    "import os\n",
    "import yaml\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# Read the config\n",
    "config_path = './config/tform_mlp_params.yaml'  \n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']    \n",
    "test_config = config['test_params']\n",
    "print(model_config)\n",
    "print(train_config)\n",
    "print(test_config)\n",
    "\n",
    "pl.seed_everything(config['seed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Load the dataset and dataloaders\n",
    "#----------------------------------------------------------\n",
    "test_data_path = './data/q_cleaned_4b_test_set.csv'\n",
    "test_dataset = dataset(train_config, model_config['block_size'], test_data_path, regularize=False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=train_config['batch_size'])\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Load pre-trained model\n",
    "#----------------------------------------------------------\n",
    "checkpoint_name = './lightning_logs/tform_mlp_model/cleaned-4-data/trained/checkpoints/epoch=316-step=9800-val_loss=0.11-loss=0.05.ckpt' \n",
    "model = TFormMLP_Lightning_v2.load_from_checkpoint(checkpoint_path=checkpoint_name, model_config=model_config, config=train_config)\n",
    "\n",
    "\n",
    "it = iter(test_loader)\n",
    "batch = next(it)\n",
    "x, y, name = batch\n",
    "print('x shape:', x.shape, ', y shape:', y.shape)\n",
    "\n",
    "y_hat, trans_out = model(x.to(model.device))\n",
    "print('y_hat shape:', y_hat.shape, ', trans_out shape', trans_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Use forward hooks to pick out the output tensors for the attention blocks\n",
    "#\n",
    "\n",
    "feats = {} #an empty dictionary\n",
    "def hook_func(m , inp ,op):\n",
    "   feats['feat'] = op.detach()\n",
    "\n",
    "# it = iter(test_loader)\n",
    "# batch = next(it)\n",
    "# x, x2, x3, y, name = batch\n",
    "y_hat, trans_out = model(x.to(model.device), x2.to(model.device), x3.to(model.device))\n",
    "\n",
    "print(feats['feat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks to pick outputs from specific layers in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
