
model_params:
  n_embd: 192
  n_layer: 6
  n_head : 8
  embd_pdrop : 0.1
  resid_pdrop : 0.1
  attn_pdrop : 0.1
  regress_head_pdrop : 0.2

  vocab_size : 25      # 20 amino acids plus, X (unknown aa), MASK, CLS, PAD, and SEP tokens
  block_size : 248     # max 246 amino acids sequence and +1 for CLS token, +1 for PAD token
  mask_prob : 0.0      # don't mask for fine tuning

  learning_rate : 0.0001
  lr_gamma: 0.9985     # for exponential learning rate decay
  betas : [0.9, 0.95]
  accelerator: 'gpu'
  devices: 1
  batch_size: 512
  num_workers: 20
  grad_norm_clip : 1.0
  num_epochs : 1000
  checkpoint_every_n_train_steps : 100
  save_top_k : 3
  monitor: 'val_loss'
  mode: 'min'
  log_dir: './lightning_logs/bert_model/cleaned-3/unfreeze_last_0_layers/'
  log_every_nsteps: 5 

  train_data_path: '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv'
  test_data_path:  '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_val_set.csv'

  inference_data_path:  '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_val_set.csv'
  # inference_data_path: '/home/mark/dev/aAlphaBio-Homework/data/alphaseq_data_hold_out.csv'

  # checkpoint for the pre-trained bert model
  checkpoint_pretrained: None #'/home/mark/dev/myBERT/lightning_logs_alphabio_homework/pretrain_oas/version_2/checkpoints/epoch=3-step=137200.ckpt'
  unfreeze_pretrained_layers: 0  # starting from the last layer in transformer, how many layers to unfreeze
                                 # 0 means all layers frozen and just the regression head is trained.

  # checkpoint for entire bert + regress head model
  checkpoint_name: '/home/mark/dev/myBERT/lightning_logs_alphabio_homework/fine_tune_from_pretrained/clean_3_dataset/base_tform_frozen/checkpoints/epoch=159-step=800.ckpt' 

  inference_results_folder: './inference_results/bert/clean_3_dataset/unfreeze_last_0_layers/'

  seed : 3407


