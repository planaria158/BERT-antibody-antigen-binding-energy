
model_params:
  block_size: 248    # 247 aa residues (max length of holdout set) + CLS token (if needed)
  vocab_size : 25    # 20 amino acids + special tokens (CLS, X, PAD, MASK, SEP)                 
  num_heads:   16 #8 
  dim_head:    64
  num_layers:  12 #6
  emb_dim:    768 #256  

train_params:
  # Either mask-language-model or regression
  train_type: 'mask_lang_model'  # 'mask_lang_model', 'regression'

  #---------------------------------------------------------------------------
  # settings specific to regression training
  #---------------------------------------------------------------------------
  loss_type:     'mse'  # 'mae', 'mse'  i.e. mean-avg-error, mean-squared-error
  sequence_regularize: False 
  seq_mask_prob:  0.05 # regularization: probability of masking an amino acid 
                       # (this masks ~10-15 residues in seq of 242 if prob = 0.05)

  freeze_base_model: False 

  #---------------------------------------------------------------------------
  # setting specific to masked language training
  #---------------------------------------------------------------------------
  mask_prob: 0.15      # for pre-training in masked-language-modeling task

  #---------------------------------------------------------------------------
  # general training settings
  #---------------------------------------------------------------------------
  tform_dropout: 0.10
  emb_dropout:   0.10
  mlp_dropout:   0.10

  accelerator : 'gpu'      # cpu, gpu, mps(macbook M2 chip)
  devices: 2
  precision: '16-mixed'    # '32-true', '16-mixed', 'bf16-mixed'
  batch_size: 36
  accumulate_grad_batches : 25
  num_workers: 10
  grad_norm_clip : 1.0
  num_epochs : 1000        # for fine-tuning use smaller number of epochs, e.g. < 10
  log_dir :        '../lightning_logs/tform_mlp_model/pretrain/big2/chain_id/paired_2_set/'
  train_data_path: '../data/oas_2/paired/paired_2_train_set.csv'
  val_data_path:   '../data/oas_2/paired/paired_2_val_set.csv'

  checkpoint_name: '../lightning_logs/tform_mlp_model/pretrain/big2/chain_id/paired_2_set/version_1/checkpoints/epoch=148-step=19600-val_loss=0.40-loss=0.39.ckpt'

  learning_rate : 0.0001  # for fine-tuning use 0.00001
  lr_gamma: 0.9995        # for exponential learning rate decay
  betas : [0.9, 0.98]
  eps : 0.000001
  checkpoint_every_n_train_steps : 100
  save_top_k : 10
  monitor: 'val_loss'  
  mode: 'min'
  log_every_nsteps: 10
  test_results_folder:      None # '../test_results/tform_mlp_model/pretrain/paired_1_set/'
  inference_results_folder: None # '../inference_results/tform_mlp_model/cleaned-4b-data/'

test_params:
  accelerator : 'mps' # cpu, gpu, mps(macbook M2 chip)
  devices: 1
  batch_size: 256
  num_workers: 2
  checkpoint_name: None
  test_data_path:  '../data/q_cleaned_4b_test_set.csv'

inference_params:
  accelerator : 'gpu' # cpu, gpu, mps(macbook M2 chip)
  devices: 1
  batch_size: 160
  num_workers: 2
  checkpoint_name:     None
  inference_data_path: '../data/alphaseq_data_hold_out.csv'

seed : 3407