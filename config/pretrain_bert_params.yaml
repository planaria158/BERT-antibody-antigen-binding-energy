
model_params:
  n_embd: 192
  n_layer: 6
  n_head : 8
  embd_pdrop : 0.1
  resid_pdrop : 0.1
  attn_pdrop : 0.1
  
  vocab_size : 25      # 20 amino acids plus, X (unknown aa), MASK, CLS, PAD, and SEP tokens
  block_size : 248     # max 246 amino acids and +1 for CLS token and +1 for SEP token
  mask_prob : 0.15
  
  learning_rate : 0.0001
  lr_gamma: 0.9985     # for exponential learning rate decay
  betas : [0.9, 0.95]
  accelerator: 'gpu'
  devices: 2
  batch_size: 200
  num_workers: 20
  grad_norm_clip : 1.0
  num_epochs : 10
  checkpoint_every_n_train_steps : 100
  save_top_k : 5
  monitor: 'loss'
  mode: 'min'
  log_dir: './lightning_logs_alphabio_homework/pretrain_oas/'
  log_every_nsteps: 100
  checkpoint_name: '/home/mark/dev/myBERT/lightning_logs_alphabio_homework/pretrain_oas/version_1/checkpoints/epoch=0-step=23900.ckpt'

  train_data_path : '/home/mark/dev/myBERT/data/oas/human_light_sars_covid/train_data.pk'
  test_data_path : '/home/mark/dev/myBERT/data/oas/human_light_sars_covid/test_data.pk'

  seed : 3407