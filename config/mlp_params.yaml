
model_params:
  block_size: 247    # 247 aa residues (no CLS or anything else)
  vocab_size]: 24    # 20 amino acids + special tokens (CLS, X, PAD, MASK)  

train_params:
  # augmentation at the sequence level, which occurs inside the dataset   
  sequence_regularize: True
  seq_flip_prob:  0.2 # regularization: probability of flipping the sequence back-to-front
  seq_mask_prob:  0.1 # regularization: probability of masking an amino acid 
                      # (this masks ~10-15 residues in seq of 242 if prob = 0.05)

  mlp_dropout : 0.25
  accelerator : 'gpu' # cpu, gpu, mps(macbook M2 chip)
  devices: 2
  batch_size: 2048
  num_workers: 5
  grad_norm_clip : 1.0
  num_epochs : 5000
  log_dir :        '../lightning_logs/mlp_model/cleaned-3b-data/'
  train_data_path: '../data/q_cleaned_3b_train_set.csv'
  val_data_path:   '../data/q_cleaned_3b_val_set.csv'
  checkpoint_name: None 
  learning_rate : 0.001
  lr_gamma: 0.9995    
  betas : [0.9, 0.95]
  checkpoint_every_n_train_steps : 3
  save_top_k : 10
  monitor: 'val_loss'
  mode: 'min'
  log_every_nsteps: 10
  test_results_folder: '../test_results/mlp_model/cleaned-3b-data/'
  inference_results_folder: '../inference_results/mlp_model/cleaned-3b-data/'

test_params:
  accelerator : 'gpu' # cpu, gpu, mps(macbook M2 chip)
  devices: 1
  batch_size: 2048
  num_workers: 2
  checkpoint_name: '../lightning_logs/mlp_model/cleaned-3b-data/trained/checkpoints/epoch=3787-step=7575-val_loss=0.41-loss=0.37.ckpt' 
  test_data_path:  '../data/q_cleaned_3b_test_set.csv'

inference_params:
  accelerator : 'gpu' # cpu, gpu, mps(macbook M2 chip)
  devices: 1
  batch_size: 2048
  num_workers: 2
  checkpoint_name: None 
  inference_data_path: '../data/alphaseq_data_hold_out.csv'

seed : 3407

