
model_params:
  block_size: 242    # can fit the max of 241 amino acids + CLS token + PAD tokens
                     # and be reshaped into a (44,44) single-channel tensor of 1's and 0's
                     # (44,44) needed to allow for 4x4 patches fed into the ViT model
                     # This process clips off the last 5 residues of the protein sequence,
                     # but those are constant in all cases, so it should not affect the model

  vocab_size]: 24    # 20 amino acids + special tokens (CLS, X, PAD, MASK)                 

  apply_augmentation: True
  seq_flip_prob:  0.1 # regularization: probability of flipping the sequence back-to-front
  seq_mask_prob:  0.05 # regularization: probability of masking an amino acid 
                       # (this masks ~10-15 residues in seq of 242 if prob = 0.05)

  num_heads: 10 #8 
  dim_head: 64
  num_layers: 12 #6
  emb_dim: 512 #256  
  tform_dropout: 0.1
  emb_dropout: 0.1
  mlp_dropout: 0.2

  accelerator : 'gpu'   # cpu, gpu, mps(macbook M2 chip)
  devices: 2
  batch_size: 52
  num_workers: 10
  grad_norm_clip : 1.0
  num_epochs : 5000
  log_dir : '/home/mark/dev/aAlphaBio-Homework/lightning_logs/tform_mlp_model/cleaned-3/'
  train_data_path: '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_train_set.csv'
  test_data_path:  '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_val_set.csv'

  # restart from checkpt
  checkpoint_name: None
  learning_rate : 0.001
  lr_gamma: 0.9995     # for exponential learning rate decay
  betas : [0.9, 0.95]
  checkpoint_every_n_train_steps : 100
  save_top_k : 50
  monitor: 'loss'  #'val_loss'
  mode: 'min'
  log_every_nsteps: 10

  # inference_data_path:  '/home/mark/dev/aAlphaBio-Homework/data/q_cleaned_3_val_set.csv'
  # inference_data_path: '/Users/markthompson/Documents/dev/a-alphaBio-homework/data/alphaseq_data_hold_out.csv'
  inference_results_folder: '/home/mark/dev/aAlphaBio-Homework/inference_results/tform_mlp_model/cleaned-3/'

  seed : 3407

