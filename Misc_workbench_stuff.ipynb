{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x.size())\n",
    "y = x.view(16)\n",
    "print(y.size())\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(z.size())\n",
    "a = x.view(-1)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3026)\n"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([0.1])\n",
    "p = torch.tensor([1])\n",
    "\n",
    "ce = - torch.sum(p * torch.log(q))\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "a = 'TNYYMYWVRQAPGQGLEWMGGINPSNGGTNFNEKFKNRVTLTTDSSTTTAYMELKSLQFDDTAVYYCARRDYRFDMGFD'\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_size': 248, 'vocab_size': 24, 'num_heads': 8, 'dim_head': 64, 'num_layers': 6, 'emb_dim': 256}\n",
      "{'loss_type': 'mse', 'sequence_regularize': True, 'seq_mask_prob': 0.05, 'mask_prob': 0, 'tform_dropout': 0.1, 'emb_dropout': 0.1, 'mlp_dropout': 0.2, 'accelerator': 'mps', 'devices': 1, 'batch_size': 256, 'num_workers': 10, 'grad_norm_clip': 1.0, 'num_epochs': 5000, 'log_dir': '../lightning_logs/tform_mlp_model/finetune/cleaned-4b-data/', 'train_data_path': '../data/q_cleaned_4b_train_set.csv', 'val_data_path': '../data/q_cleaned_4b_val_set.csv', 'freeze_encoder': True, 'checkpoint_name': '../lightning_logs/tform_mlp_model/pretrain/high_q_pretrain_set/mask_0.15/checkpoints/epoch=42-step=3000-val_loss=0.05-loss=0.06.ckpt', 'learning_rate': 0.0001, 'lr_gamma': 0.9995, 'betas': [0.9, 0.95], 'checkpoint_every_n_train_steps': 100, 'save_top_k': 10, 'monitor': 'val_loss', 'mode': 'min', 'log_every_nsteps': 10, 'test_results_folder': '../test_results/tform_mlp_model/finetune/cleaned-4b-data/', 'inference_results_folder': '../inference_results/tform_mlp_model/cleaned-4b-data/'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3407"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the config\n",
    "config_path = './config/tform_params.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']    \n",
    "\n",
    "print(model_config)\n",
    "print(train_config)\n",
    "pl.seed_everything(config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class scFv_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "        Dataset class for scFv sequence, Kd data\n",
    "\n",
    "        Args:\n",
    "            config: dict with configuration parameters\n",
    "            csv_file_path: path to the csv file\n",
    "            skiprows: number of rows to skip at the beginning of the file\n",
    "            inference: if True, the dataset is used for inference\n",
    "            regularize: if True, the dataset is used for training and data augmentation/regularization is applied\n",
    "\n",
    "        Returns:\n",
    "            dix: tensor of encoded amino acid sequence\n",
    "            mask: tensor of masked amino acid sequence\n",
    "            affinity: tensor of affinity values\n",
    "            name: name of the sequence\n",
    "\n",
    "        If inference is True, affinity is set to 0 and name is the name of the sequence\n",
    "        If regularize is True, the sequence is regularized with a small percentage of the amino acids masked\n",
    "\n",
    "        This dataset operates in two modes:\n",
    "        1. Regression training\n",
    "           The original sequence with a small percentage of the amino acids masked\n",
    "           sequence will look something like: [aa1, aa2, MASK, aa4, MASK, aa6, ...PAD,..PAD]\n",
    "           mask will be None\n",
    "\n",
    "        2. Masked language model training\n",
    "           The masked language model training sequence with a small percentage of the amino acids masked\n",
    "           sequence will look something like:           [CLS, aa, aa, MASK, aa, MASK, aa, ...PAD,..PAD]\n",
    "           corresponding mask will look something like: [0,   0,   0,   aa,  0,   aa, 0, ...0, 0]\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config, block_size, csv_file_path, skiprows=0, inference=False, regularize=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.block_size = block_size\n",
    "        self.inference = inference\n",
    "        self.regularize = regularize # sequence flipping etc...\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 amino acids + special tokens (CLS, X, PAD, MASK) \n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', \n",
    "                      'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        self.first_aa_idx = 1\n",
    "        self.last_aa_idx = len(self.chars) - 4\n",
    "        print('vocabulary:', self.chars)\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size' % (data_size, vocab_size))\n",
    "\n",
    "        # encoding and decoding residues\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def get_random_aa_tokens(self, length):\n",
    "        return torch.tensor([self.stoi[aa] for aa in random.choices(self.chars[self.first_aa_idx:self.last_aa_idx+1], k=length)], dtype=torch.long)\n",
    "\n",
    "    #-------------------------------------------------------\n",
    "    # Create the mask for masked language model training\n",
    "    # and the corresponding modified dix sequence\n",
    "    #-------------------------------------------------------\n",
    "    def create_mask(self, dix):\n",
    "        mask = torch.zeros_like(dix)\n",
    "\n",
    "        # prepend the CLS token to the sequence\n",
    "        dix = torch.cat((torch.tensor([self.stoi['CLS']], dtype=torch.long), dix))\n",
    "\n",
    "        # training will be masked language model\n",
    "        if self.config['mask_prob'] > 0:\n",
    "\n",
    "            # get number of tokens to mask\n",
    "            n_pred = max(1, int(round(self.block_size * self.config['mask_prob'])))\n",
    "\n",
    "            # indices of the tokens that will be masked (a random permutation of n_pred of the tokens)\n",
    "            masked_idx = torch.randperm(len(dix)-1, dtype=torch.long)[:n_pred]\n",
    "            masked_idx += 1  # so we never mask the CLS token\n",
    "            mask = torch.zeros_like(dix)\n",
    "\n",
    "            # copy the actual tokens to be masked, to the mask\n",
    "            mask[masked_idx] = dix[masked_idx]\n",
    "\n",
    "            # Standard 80-10-10 corruption strategy\n",
    "            # with 80% probability, change to MASK token\n",
    "            # with 10% probability, change to random aa token\n",
    "            # with 10% probability, keep the same token\n",
    "            # This has been disputed by Wettig et. al. https://arxiv.org/pdf/2202.08005\n",
    "            # Therefore, I'll not do it this way.\n",
    "            p8 = int(math.ceil(n_pred*0.8))\n",
    "            p9 = int(math.ceil(n_pred*0.9))\n",
    "            print('p8:', p8, 'p9:', p9, 'n_pred:', n_pred)\n",
    "            mask_token_idxs = masked_idx[0:p8]\n",
    "            rand_aa_idxs = masked_idx[p8:p9]\n",
    "            keep_idxs = masked_idx[p9:]\n",
    "            assert(masked_idx.shape[0] == mask_token_idxs.shape[0] + rand_aa_idxs.shape[0] + keep_idxs.shape[0])            \n",
    "            dix[mask_token_idxs] = self.stoi['MASK']    \n",
    "            dix[rand_aa_idxs] = self.get_random_aa_tokens(rand_aa_idxs.shape[0])\n",
    "\n",
    "            # Simply replace all masked tokens with the MASK token\n",
    "            # dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        return dix, mask\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] \n",
    "\n",
    "\n",
    "    #-------------------------------------------------------\n",
    "    # Get the sequence, mask, affinity and name data\n",
    "    #-------------------------------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "\n",
    "        # apologies: next couple lines are overly dataset-specific\n",
    "        if self.inference == False: # training or test mode\n",
    "            Kd = self.df.loc[idx, 'Kd'] if self.inference == False else 0\n",
    "            assert not math.isnan(Kd), 'Kd is nan'\n",
    "            name = 'none'\n",
    "        else:\n",
    "            Kd = 0 # inference mode - Kd is not available\n",
    "            name = self.df.loc[idx, 'description_a']\n",
    "\n",
    "        assert Kd >= 0.0, 'not allowing for negative affinities'\n",
    "\n",
    "        # get a randomly located block_size substring from the sequence\n",
    "        if len(seq) <= self.block_size:\n",
    "            chunk = seq\n",
    "        else:\n",
    "            start_idx = np.random.randint(0, len(seq) - (self.block_size))\n",
    "            chunk = seq[start_idx:start_idx + self.block_size]\n",
    "\n",
    "        # encode the string\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "\n",
    "        # Sequence-level regularization & augmentation can be done here\n",
    "        # Only do this if we are training for regression \n",
    "        # Do not do this for masked language model training\n",
    "        if self.config['mask_prob'] == 0 and self.regularize:\n",
    "            # mask a small perentage of the amino acids with the MASK token\n",
    "            # acts like a dropout.  This is NOT the same as the masked language model training\n",
    "            # and the model should not be trained with this regularization during masked \n",
    "            # language model training\n",
    "            if self.config['seq_mask_prob'] > 0.0:\n",
    "                num_2_mask = max(0, int(round((dix.shape[0])*self.config['seq_mask_prob'])))\n",
    "                masked_idx = torch.randperm((dix.shape[0]), dtype=torch.long)[:num_2_mask]\n",
    "                dix[masked_idx] = self.stoi['MASK']\n",
    "\n",
    "        # create the mask for the masked language model training\n",
    "        # and the corresponding modified dix sequence\n",
    "        dix, mask = self.create_mask(dix)\n",
    "\n",
    "        # Append with PAD tokens if necessary\n",
    "        if dix.shape[0] < self.block_size:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.block_size - len(dix)), dtype=torch.long)))\n",
    "            if mask != None:\n",
    "                mask = torch.cat((mask, torch.tensor([0] * (self.block_size - len(mask)), dtype=torch.long)))\n",
    "\n",
    "\n",
    "        return dix, mask, torch.tensor([Kd], dtype=torch.float32), name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = './data/q_cleaned_4b_train_set.csv'\n",
    "dataset = scFv_Dataset(train_config, model_config['block_size'], csv_file_path, regularize=train_config['sequence_regularize'])\n",
    "\n",
    "x, mask, kd, name = dataset.__getitem__(0)\n",
    "print('x shape:', x.shape, 'mask shape:', mask.shape, ', kd:', kd, ', name:', name)\n",
    "\n",
    "print('x[-5:]:', x[-5:])\n",
    "print()\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=256) #config['batch_size'])\n",
    "it = iter(train_loader)\n",
    "x, mask, kd, _ = next(it)\n",
    "print('x.dtype:', x.dtype, ', x.shape:', x.shape, ', mask.shape:', mask.shape, ', kd.shape:', kd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets.scFv_dataset import scFv_Dataset\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Simple wrapper Dataset to turn output from the scFv dataset\n",
    "# into a B&W image for use in a CNN model\n",
    "#--------------------------------------------------------\n",
    "class CNN_Dataset_BGR(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, transform=None, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        self.img_shape = config['image_shape']\n",
    "        self.transform = transform\n",
    "         \n",
    "        chars = self.scFv_dataset.chars\n",
    "        groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "                'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "        \n",
    "        # for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "        group_encodings = { 'none'    : int('11001100', base=2), \n",
    "                            'polar'   : int('00110011', base=2),\n",
    "                            'nonpolar': int('01100110', base=2), \n",
    "                            'pos'     : int('01010101', base=2),\n",
    "                            'neg'     : int('10101010', base=2)} \n",
    "        \n",
    "        print('group_encodings:', group_encodings)\n",
    "\n",
    "        # map encoded sequence to groups\n",
    "        self.i_to_grp = {self.scFv_dataset.stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "        # The relative mutation frequence for each amino acid position in the scFv sequences over the entire clean_3 dataset\n",
    "        # This fixed-array is 241 elements long. (I clipped off the last 5 residues from the 246 residue sequences for the VIT model)\n",
    "        self.rel_mutation_freq = torch.tensor([ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7778,\n",
    "                                                0.9444, 0.9444, 1.0000, 1.0000, 1.0000, 0.9444, 1.0000, 1.0000, 0.8889,\n",
    "                                                0.5556, 0.5000, 0.2222, 0.3333, 0.2778, 0.6111, 0.4444, 0.5556, 1.0000,\n",
    "                                                0.8333, 0.8333, 0.8333, 0.9444, 0.7778, 0.8333, 0.6111, 1.0000, 0.8889,\n",
    "                                                0.3333, 0.9444, 0.8889, 0.1111, 0.3889, 0.9444, 0.2778, 0.9444, 0.8333,\n",
    "                                                0.5000, 1.0000, 1.0000, 1.0000, 0.9444, 0.6111, 0.6111, 0.7778, 0.2778,\n",
    "                                                0.8889, 0.3889, 0.9444, 1.0000, 0.3889, 0.9444, 1.0000, 0.9444, 0.2222,\n",
    "                                                0.7778, 0.5556, 0.8889, 0.2222, 0.7778, 0.6111, 0.6667, 0.8333, 0.8333,\n",
    "                                                1.0000, 1.0000, 0.8889, 0.8333, 0.8333, 0.9444, 0.7222, 0.9444, 0.9444,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]) \n",
    "        \n",
    "        self.mutation_freq_encoded = self.rel_mutation_freq * 255\n",
    "        self.mutation_freq_encoded = torch.ceil(self.mutation_freq_encoded).to(torch.long)\n",
    "        print('min mutation freq:', torch.min(self.mutation_freq_encoded), 'max mutation freq:', torch.max(self.mutation_freq_encoded))\n",
    "        print('some mutation_freq_encoded values:', self.mutation_freq_encoded[60:70])\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _encode_channel(self, x, shape):\n",
    "        d = ''.join([self._bin(val) for val in x])\n",
    "        d = [int(x) for x in d] # turn d into a list of integers, one for each bit\n",
    "        t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for shape matrix\n",
    "        t = t.reshape(shape)\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, Kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "\n",
    "        # The residue encoding channel\n",
    "        ch_1 = self._encode_channel(dix, self.img_shape)\n",
    "        print('ch_1:', ch_1.shape, torch.min(ch_1), torch.max(ch_1))\n",
    "\n",
    "        # The residue group encoding channel\n",
    "        dix_grp = torch.tensor([self.i_to_grp[i] for i in dix.numpy().tolist()], dtype=torch.long)\n",
    "        ch_2 = self._encode_channel(dix_grp, self.img_shape)\n",
    "        print('ch_2:', ch_2.shape, torch.min(ch_2), torch.max(ch_2))   \n",
    "\n",
    "        # The mutation frequency channel; anything not an amino acid gets a zero.\n",
    "        ch3_in = torch.zeros_like(dix)\n",
    "        # First aa is always position 1 (0 is a CLS token)\n",
    "        ch3_in[1:len(self.mutation_freq_encoded)+1] = self.mutation_freq_encoded\n",
    "        ch_3 = self._encode_channel(ch3_in, self.img_shape)\n",
    "        print('ch_3:', ch_3.shape, torch.min(ch_3), torch.max(ch_3))   \n",
    "\n",
    "        # stack the 3 channels into a bgr image\n",
    "        bgr_img = torch.stack((ch_1, ch_2, ch_3), dim=0) * 255\n",
    "\n",
    "        if self.transform:\n",
    "            bgr_img = self.transform(bgr_img)\n",
    "            \n",
    "        # Normalize image [-1, 1]\n",
    "        bgr_img = (bgr_img - 127.5)/127.5\n",
    "\n",
    "\n",
    "        return bgr_img, kd, ch_1, ch_2, ch_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "\n",
    "# train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "#                             RandomHorizontalFlip(p=0.25),\n",
    "#                             RandomVerticalFlip(p=0.25)])\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset_BGR(config, train_data_path) #, train_transforms)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, kd, ch_1, ch_2, ch_3 = train_dataset.__getitem__(105)\n",
    "print(img.dtype)\n",
    "# change the order of the channels to be (H, W) instead of (C, H, W)\n",
    "rgb_img = img.permute(1, 2, 0)\n",
    "\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "# plt.imshow(rgb_img) #, cmap='gray')\n",
    "\n",
    "plt.imshow(ch_3, cmap='gray')\n",
    "\n",
    "print(torch.min(rgb_img), ', ', torch.max(rgb_img), ', ', torch.mean(rgb_img), ', ', torch.std(rgb_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Examine other data that may be added as input channels to Vision Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the amino acids into their usual groups\n",
    "# polar, nonpolar, positively charged, negatively charged, or none (i.e. CLS, SEP, PAD)\n",
    "#\n",
    "# 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "# 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "         'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "print('\\nvocabulary:', chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "group_encodings = { 'none'    : int('00101000', base=2), \n",
    "                    'polar'   : int('00110011', base=2),\n",
    "                    'nonpolar': int('11001100', base=2), \n",
    "                    'pos'     : int('01010101', base=2),\n",
    "                    'neg'     : int('10101010', base=2)} \n",
    "\n",
    "print('group_encodings:', group_encodings)\n",
    "\n",
    "# maps amino acid to group                     \n",
    "s_to_grp = {ch:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "# maps encoded residue to group\n",
    "i_to_grp = {stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "print('\\ns_to_grp:', s_to_grp)\n",
    "print('\\ni_to_grp:', i_to_grp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _bin(x):\n",
    "    return format(x, '08b')\n",
    "\n",
    "def _encode_channel(x, shape=(48,48)):\n",
    "    d = ''.join([_bin(x[i]) for i in x.numpy()])\n",
    "    # turn d into a list of integers, one for each bit\n",
    "    d = [int(x) for x in d]    \n",
    "    t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "    t = t.reshape(shape)\n",
    "    # t = t.unsqueeze(0) # add channel dimension\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a heat-map for the variability of each position in the sequence?\n",
    "# That somehow changes with each sequence?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
