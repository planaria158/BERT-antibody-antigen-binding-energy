{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-AlphaBio homework \n",
    "### misc. futzing about.... prob messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'TNYYMYWVRQAPGQGLEWMGGINPSNGGTNFNEKFKNRVTLTTDSSTTTAYMELKSLQFDDTAVYYCARRDYRFDMGFD'\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the config\n",
    "config_path = './config/tform_mlp_params_v3.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "model_config = config['model_params']\n",
    "train_config = config['train_params']    \n",
    "\n",
    "print(model_config)\n",
    "print(train_config)\n",
    "pl.seed_everything(config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class scFv_Dataset_v3(Dataset):\n",
    "    \"\"\"\n",
    "        Dataset class for scFv sequence, Kd data\n",
    "        version 3: this class only uses positions 25 - 110 of each sequence.\n",
    "        This corresponds to a block length = 85\n",
    "\n",
    "        Args:\n",
    "            config: dict with configuration parameters\n",
    "            csv_file_path: path to the csv file\n",
    "            skiprows: number of rows to skip at the beginning of the file\n",
    "            inference: if True, the dataset is used for inference\n",
    "            regularize: if True, the dataset is used for training and data augmentation/regularization is applied\n",
    "    \"\"\"\n",
    "    def __init__(self, config, block_size, start_seq_idx, csv_file_path, skiprows=0, inference=False, regularize=False):  \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.block_size = block_size\n",
    "        self.start_seq_idx = start_seq_idx # always start on this index (i.e 25)\n",
    "        self.inference = inference\n",
    "        self.regularize = regularize # sequence flipping etc...\n",
    "        print('reading the data from:', csv_file_path)\n",
    "        self.df = pd.read_csv(csv_file_path, skiprows=skiprows)\n",
    "        \n",
    "        # 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "        # 'X' is a special token for unknown amino acids, CLS token is for classification, and PAD for padding\n",
    "        self.chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', \n",
    "                      'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "        self.first_aa = self.chars[1]\n",
    "        self.last_aa = self.chars[20]\n",
    "        print('vocabulary:', self.chars)\n",
    "        data_size, vocab_size = self.df.shape[0], len(self.chars)\n",
    "        print('data has %d rows, %d vocab size (unique).' % (data_size, vocab_size))\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # aa groups: group name for each position in the self.chars array above\n",
    "        self.aa_groups = ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', \n",
    "                          'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "                          'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', \n",
    "                          'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "        \n",
    "        self.groups = ['none', 'nonpolar', 'neg', 'pos', 'polar']\n",
    "        print('aa groups:', self.aa_groups)\n",
    "\n",
    "        # The relative variability frequence for each amino acid position in the scFv sequences over the entire clean_3 dataset\n",
    "        # This fixed-array is 247 (246 residues + an extra 0.0000 added for holdout set)\n",
    "        #\n",
    "        # A better way to do this would have been to pre-computed for each dataset separately.\n",
    "        raw_pos_variability = torch.tensor([ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7778,\n",
    "                                             0.9444, 0.9444, 1.0000, 1.0000, 1.0000, 0.9444, 1.0000, 1.0000, 0.8889,\n",
    "                                             0.5556, 0.5000, 0.2222, 0.3333, 0.2778, 0.6111, 0.4444, 0.5556, 1.0000,\n",
    "                                             0.8333, 0.8333, 0.8333, 0.9444, 0.7778, 0.8333, 0.6111, 1.0000, 0.8889,\n",
    "                                             0.3333, 0.9444, 0.8889, 0.1111, 0.3889, 0.9444, 0.2778, 0.9444, 0.8333,\n",
    "                                             0.5000, 1.0000, 1.0000, 1.0000, 0.9444, 0.6111, 0.6111, 0.7778, 0.2778,\n",
    "                                             0.8889, 0.3889, 0.9444, 1.0000, 0.3889, 0.9444, 1.0000, 0.9444, 0.2222,\n",
    "                                             0.7778, 0.5556, 0.8889, 0.2222, 0.7778, 0.6111, 0.6667, 0.8333, 0.8333,\n",
    "                                             1.0000, 1.0000, 0.8889, 0.8333, 0.8333, 0.9444, 0.7222, 0.9444, 0.9444,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, \n",
    "                                             0.0000, 0.0000, 0.0000, 0.0000]) \n",
    "        \n",
    "        # Map the raw_pos_variability into 10 buckets based on value\n",
    "        var_buckets = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "        def get_bucket(n):\n",
    "            assert(n >= 0 and n <= 1), \"make sure n is in the range [0,1.0]\"\n",
    "            for i, k in enumerate(var_buckets):\n",
    "                if n <= k:\n",
    "                    return i\n",
    "            return 0        # assign each variability value to a bucket\n",
    "        \n",
    "        self.enc_pos_variability = [get_bucket(v) for v in raw_pos_variability]\n",
    "\n",
    "        print('a sample of some raw variability values:', raw_pos_variability[60:65])\n",
    "        print('and their corresponding encoded values :', self.enc_pos_variability[60:65])\n",
    "\n",
    "\n",
    "        # encoding and decoding residues\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        # encoding decoding groups\n",
    "        self.groupstoi = { group:i for i,group in enumerate(self.aa_groups) }\n",
    "        self.groupitos = { i:group for i,group in enumerate(self.aa_groups) }\n",
    "        self.gtoi = { group:i for i,group in enumerate(self.groups) }\n",
    "        self.itog = { i:group for i,group in enumerate(self.groups) }\n",
    "\n",
    "    def encode_aa(self, aa_name):\n",
    "        return self.stoi[aa_name]\n",
    "    \n",
    "    def decode_aa(self, aa_idx):\n",
    "        assert aa_idx < self.vocab_size, 'aa_idx is out of bounds'\n",
    "        return self.itos[aa_idx]\n",
    "    \n",
    "    # Get the group encoding for a given amino acid\n",
    "    def encode_aa_to_group(self, aa_name):\n",
    "        aa_idx = self.stoi[aa_name]\n",
    "        group_name = self.groupitos[aa_idx]\n",
    "        group_enc = self.gtoi[group_name]\n",
    "        return group_enc\n",
    "    \n",
    "    def decode_group(self, group_idx):\n",
    "        return self.itog[group_idx]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" \n",
    "            Returns sequence encoding, group encoding, affinity\n",
    "        \"\"\"\n",
    "        seq = self.df.loc[idx, 'sequence_a']\n",
    "\n",
    "        # apologies: next couple lines are overly dataset-specific\n",
    "        if self.inference == False: # training or test mode\n",
    "            Kd = self.df.loc[idx, 'Kd']\n",
    "            assert not math.isnan(Kd), 'Kd is nan'\n",
    "            name = 'none'\n",
    "        else:\n",
    "            Kd = 0 # inference mode - Kd is not available\n",
    "            name = self.df.loc[idx, 'description_a']\n",
    "\n",
    "        assert Kd >= 0.0, 'affinity cannot be negative'\n",
    "\n",
    "        # get a randomly located block_size substring from the sequence\n",
    "        assert(len(seq) > self.block_size), 'sequence is shorter than block_size'\n",
    "        # if len(seq) <= self.block_size:\n",
    "        #     chunk = seq\n",
    "        # else:\n",
    "        start_idx = self.start_seq_idx  #np.random.randint(0, len(seq) - (self.block_size))\n",
    "        chunk = seq[start_idx:start_idx + self.block_size]\n",
    "\n",
    "        # encode residues, residues' groups, and position variability\n",
    "        dix = torch.tensor([self.stoi[s] for s in chunk], dtype=torch.long)\n",
    "        gix = torch.tensor([self.encode_aa_to_group(s) for s in chunk], dtype=torch.long)\n",
    "        vix = torch.tensor(self.enc_pos_variability[:len(dix)], dtype=torch.long)\n",
    "\n",
    "        # some sequence-level regularization & augmentation can be done here\n",
    "        if self.regularize:\n",
    "            # occasionally flip the aa sequences back-to-front as a regularization technique \n",
    "            dix = torch.flip(dix, [0]) if (random.random() < self.config['seq_flip_prob']) else dix\n",
    "            gix = torch.flip(gix, [0]) if (random.random() < self.config['seq_flip_prob']) else gix\n",
    "            vix = torch.flip(vix, [0]) if (random.random() < self.config['seq_flip_prob']) else vix\n",
    "\n",
    "            # mask a small perentage of the amino acids with the MASK token\n",
    "            # acts like a dropout\n",
    "            if self.config['seq_mask_prob'] > 0.0:\n",
    "                num_2_mask = max(0, int(round((dix.shape[0])*self.config['seq_mask_prob'])))\n",
    "                masked_idx = torch.randperm((dix.shape[0]), dtype=torch.long)[:num_2_mask]\n",
    "                dix[masked_idx] = self.stoi['MASK']\n",
    "                gix[masked_idx] = self.gtoi['none']\n",
    "                vix[masked_idx] = 0  # ?? not sure about this...\n",
    "\n",
    "\n",
    "        # pad the end with PAD tokens if necessary\n",
    "        if dix.shape[0] < self.block_size:\n",
    "            dix = torch.cat((dix, torch.tensor([self.stoi['PAD']] * (self.block_size - len(dix)), dtype=torch.long)))\n",
    "            gix = torch.cat((gix, torch.tensor([self.gtoi['none']] * (self.block_size - len(gix)), dtype=torch.long)))\n",
    "            vix = torch.cat((vix, torch.tensor([0] * (self.block_size - len(vix)), dtype=torch.long)))\n",
    "\n",
    "        return dix, gix, vix, torch.tensor([Kd], dtype=torch.float32), name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = scFv_Dataset_v3(train_config, model_config['block_size'], model_config['start_seq_idx'], train_config['train_data_path'], \n",
    "                          regularize=train_config['sequence_regularize'])\n",
    "\n",
    "x, x2, x3, kd, name = dataset.__getitem__(0)\n",
    "print('x shape:', x.shape, ', x2 shape:', x2.shape, ', x3 shape:', x3.shape, ', kd:', kd, ', name:', name)\n",
    "print('x.dtype:', x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)\n",
    "x, kd = next(it)\n",
    "print('x.dtype:', x.dtype, ', x.shape:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from datasets.scFv_dataset import scFv_Dataset\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Simple wrapper Dataset to turn output from the scFv dataset\n",
    "# into a B&W image for use in a CNN model\n",
    "#--------------------------------------------------------\n",
    "class CNN_Dataset_BGR(Dataset):\n",
    "    \"\"\"\n",
    "    Emits 2D B&W images and binding energies\n",
    "    \"\"\"\n",
    "    def __init__(self, config, csv_file_path, transform=None, skiprows=0, inference=False):  \n",
    "        super().__init__()\n",
    "        self.scFv_dataset = scFv_Dataset(config, csv_file_path, skiprows, inference)\n",
    "        self.config = config\n",
    "        self.img_shape = config['image_shape']\n",
    "        self.transform = transform\n",
    "         \n",
    "        chars = self.scFv_dataset.chars\n",
    "        groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "                'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "        \n",
    "        # for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "        group_encodings = { 'none'    : int('11001100', base=2), \n",
    "                            'polar'   : int('00110011', base=2),\n",
    "                            'nonpolar': int('01100110', base=2), \n",
    "                            'pos'     : int('01010101', base=2),\n",
    "                            'neg'     : int('10101010', base=2)} \n",
    "        \n",
    "        print('group_encodings:', group_encodings)\n",
    "\n",
    "        # map encoded sequence to groups\n",
    "        self.i_to_grp = {self.scFv_dataset.stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "        # The relative mutation frequence for each amino acid position in the scFv sequences over the entire clean_3 dataset\n",
    "        # This fixed-array is 241 elements long. (I clipped off the last 5 residues from the 246 residue sequences for the VIT model)\n",
    "        self.rel_mutation_freq = torch.tensor([ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7778,\n",
    "                                                0.9444, 0.9444, 1.0000, 1.0000, 1.0000, 0.9444, 1.0000, 1.0000, 0.8889,\n",
    "                                                0.5556, 0.5000, 0.2222, 0.3333, 0.2778, 0.6111, 0.4444, 0.5556, 1.0000,\n",
    "                                                0.8333, 0.8333, 0.8333, 0.9444, 0.7778, 0.8333, 0.6111, 1.0000, 0.8889,\n",
    "                                                0.3333, 0.9444, 0.8889, 0.1111, 0.3889, 0.9444, 0.2778, 0.9444, 0.8333,\n",
    "                                                0.5000, 1.0000, 1.0000, 1.0000, 0.9444, 0.6111, 0.6111, 0.7778, 0.2778,\n",
    "                                                0.8889, 0.3889, 0.9444, 1.0000, 0.3889, 0.9444, 1.0000, 0.9444, 0.2222,\n",
    "                                                0.7778, 0.5556, 0.8889, 0.2222, 0.7778, 0.6111, 0.6667, 0.8333, 0.8333,\n",
    "                                                1.0000, 1.0000, 0.8889, 0.8333, 0.8333, 0.9444, 0.7222, 0.9444, 0.9444,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "                                                0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]) \n",
    "        \n",
    "        self.mutation_freq_encoded = self.rel_mutation_freq * 255\n",
    "        self.mutation_freq_encoded = torch.ceil(self.mutation_freq_encoded).to(torch.long)\n",
    "        print('min mutation freq:', torch.min(self.mutation_freq_encoded), 'max mutation freq:', torch.max(self.mutation_freq_encoded))\n",
    "        print('some mutation_freq_encoded values:', self.mutation_freq_encoded[60:70])\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.scFv_dataset.vocab_size\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.config['block_size']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scFv_dataset.__len__()\n",
    "\n",
    "    def _bin(self, x):\n",
    "        return format(x, '08b')\n",
    "\n",
    "    def _encode_channel(self, x, shape):\n",
    "        d = ''.join([self._bin(val) for val in x])\n",
    "        d = [int(x) for x in d] # turn d into a list of integers, one for each bit\n",
    "        t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for shape matrix\n",
    "        t = t.reshape(shape)\n",
    "        return t\n",
    "\n",
    "    \"\"\" Returns image, Kd pairs used for CNN training \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        dix, kd = self.scFv_dataset.__getitem__(idx)\n",
    "\n",
    "        # The residue encoding channel\n",
    "        ch_1 = self._encode_channel(dix, self.img_shape)\n",
    "        print('ch_1:', ch_1.shape, torch.min(ch_1), torch.max(ch_1))\n",
    "\n",
    "        # The residue group encoding channel\n",
    "        dix_grp = torch.tensor([self.i_to_grp[i] for i in dix.numpy().tolist()], dtype=torch.long)\n",
    "        ch_2 = self._encode_channel(dix_grp, self.img_shape)\n",
    "        print('ch_2:', ch_2.shape, torch.min(ch_2), torch.max(ch_2))   \n",
    "\n",
    "        # The mutation frequency channel; anything not an amino acid gets a zero.\n",
    "        ch3_in = torch.zeros_like(dix)\n",
    "        # First aa is always position 1 (0 is a CLS token)\n",
    "        ch3_in[1:len(self.mutation_freq_encoded)+1] = self.mutation_freq_encoded\n",
    "        ch_3 = self._encode_channel(ch3_in, self.img_shape)\n",
    "        print('ch_3:', ch_3.shape, torch.min(ch_3), torch.max(ch_3))   \n",
    "\n",
    "        # stack the 3 channels into a bgr image\n",
    "        bgr_img = torch.stack((ch_1, ch_2, ch_3), dim=0) * 255\n",
    "\n",
    "        if self.transform:\n",
    "            bgr_img = self.transform(bgr_img)\n",
    "            \n",
    "        # Normalize image [-1, 1]\n",
    "        bgr_img = (bgr_img - 127.5)/127.5\n",
    "\n",
    "\n",
    "        return bgr_img, kd, ch_1, ch_2, ch_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from datasets.scFv_dataset import scFv_Dataset as dataset\n",
    "from torchvision.transforms.v2 import Resize, Compose, ToDtype, RandomHorizontalFlip, RandomVerticalFlip \n",
    "\n",
    "# train_transforms = Compose([ToDtype(torch.float32, scale=False),\n",
    "#                             RandomHorizontalFlip(p=0.25),\n",
    "#                             RandomVerticalFlip(p=0.25)])\n",
    "\n",
    "train_data_path = config['train_data_path']  \n",
    "train_dataset = CNN_Dataset_BGR(config, train_data_path) #, train_transforms)\n",
    "print(train_dataset.__len__())\n",
    "config['vocab_size'] = train_dataset.get_vocab_size()\n",
    "print('config[vocab_size]:', config['vocab_size'], ', config[block_size]:', config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=config['batch_size'], num_workers=config['num_workers'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, kd, ch_1, ch_2, ch_3 = train_dataset.__getitem__(105)\n",
    "print(img.dtype)\n",
    "# change the order of the channels to be (H, W) instead of (C, H, W)\n",
    "rgb_img = img.permute(1, 2, 0)\n",
    "\n",
    "print('img shape:', img.shape, ', kd:', kd)\n",
    "# plt.imshow(rgb_img) #, cmap='gray')\n",
    "\n",
    "plt.imshow(ch_3, cmap='gray')\n",
    "\n",
    "print(torch.min(rgb_img), ', ', torch.max(rgb_img), ', ', torch.mean(rgb_img), ', ', torch.std(rgb_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Examine other data that may be added as input channels to Vision Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the amino acids into their usual groups\n",
    "# polar, nonpolar, positively charged, negatively charged, or none (i.e. CLS, SEP, PAD)\n",
    "#\n",
    "# 20 naturally occuring amino acids in human proteins plus MASK token, \n",
    "# 'X' is a special token for unknown amino acids, and CLS token is for classification, and PAD for padding\n",
    "chars = ['CLS', 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', 'MASK', 'PAD']\n",
    "groups= ['none', 'nonpolar', 'nonpolar', 'neg', 'neg', 'nonpolar', 'nonpolar', 'pos', 'nonpolar', 'pos', 'nonpolar', 'nonpolar', 'neg', \n",
    "         'nonpolar', 'neg', 'pos', 'polar', 'polar', 'nonpolar', 'nonpolar', 'polar', 'none', 'none', 'none']\n",
    "print('\\nvocabulary:', chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# for VIT, since the residue encodings are spread over 8-bits, assign encodings to groups that spread across the 8-bits\n",
    "group_encodings = { 'none'    : int('00101000', base=2), \n",
    "                    'polar'   : int('00110011', base=2),\n",
    "                    'nonpolar': int('11001100', base=2), \n",
    "                    'pos'     : int('01010101', base=2),\n",
    "                    'neg'     : int('10101010', base=2)} \n",
    "\n",
    "print('group_encodings:', group_encodings)\n",
    "\n",
    "# maps amino acid to group                     \n",
    "s_to_grp = {ch:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "# maps encoded residue to group\n",
    "i_to_grp = {stoi[ch]:group_encodings[i] for ch,i in zip(chars, groups)} \n",
    "\n",
    "print('\\ns_to_grp:', s_to_grp)\n",
    "print('\\ni_to_grp:', i_to_grp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _bin(x):\n",
    "    return format(x, '08b')\n",
    "\n",
    "def _encode_channel(x, shape=(48,48)):\n",
    "    d = ''.join([_bin(x[i]) for i in x.numpy()])\n",
    "    # turn d into a list of integers, one for each bit\n",
    "    d = [int(x) for x in d]    \n",
    "    t = torch.tensor(d[:(shape[0]*shape[1])], dtype=torch.float32) # this is for 46,46 matrix\n",
    "    t = t.reshape(shape)\n",
    "    # t = t.unsqueeze(0) # add channel dimension\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a heat-map for the variability of each position in the sequence?\n",
    "# That somehow changes with each sequence?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avm-dvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
